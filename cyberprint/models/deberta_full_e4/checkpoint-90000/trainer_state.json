{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 90000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0044444444444444444,
      "grad_norm": 2.1804146766662598,
      "learning_rate": 1.9978e-05,
      "loss": 0.5597,
      "step": 100
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 5.5219526290893555,
      "learning_rate": 1.995577777777778e-05,
      "loss": 0.3944,
      "step": 200
    },
    {
      "epoch": 0.013333333333333334,
      "grad_norm": 6.493681907653809,
      "learning_rate": 1.9933555555555556e-05,
      "loss": 0.2945,
      "step": 300
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 4.978532314300537,
      "learning_rate": 1.9911333333333337e-05,
      "loss": 0.2361,
      "step": 400
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 5.394551753997803,
      "learning_rate": 1.988911111111111e-05,
      "loss": 0.2332,
      "step": 500
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 4.090579032897949,
      "learning_rate": 1.9866888888888892e-05,
      "loss": 0.2292,
      "step": 600
    },
    {
      "epoch": 0.03111111111111111,
      "grad_norm": 0.3639516830444336,
      "learning_rate": 1.9844666666666667e-05,
      "loss": 0.2243,
      "step": 700
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 4.854381084442139,
      "learning_rate": 1.9822444444444448e-05,
      "loss": 0.1969,
      "step": 800
    },
    {
      "epoch": 0.04,
      "grad_norm": 2.182312488555908,
      "learning_rate": 1.9800222222222222e-05,
      "loss": 0.2019,
      "step": 900
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 1.7353620529174805,
      "learning_rate": 1.9778000000000003e-05,
      "loss": 0.1778,
      "step": 1000
    },
    {
      "epoch": 0.04888888888888889,
      "grad_norm": 1.7962411642074585,
      "learning_rate": 1.975577777777778e-05,
      "loss": 0.1788,
      "step": 1100
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.15538451075553894,
      "learning_rate": 1.9733555555555558e-05,
      "loss": 0.1584,
      "step": 1200
    },
    {
      "epoch": 0.057777777777777775,
      "grad_norm": 4.858280658721924,
      "learning_rate": 1.9711333333333336e-05,
      "loss": 0.2082,
      "step": 1300
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 3.107184410095215,
      "learning_rate": 1.9689111111111113e-05,
      "loss": 0.1585,
      "step": 1400
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 4.846212863922119,
      "learning_rate": 1.966688888888889e-05,
      "loss": 0.1815,
      "step": 1500
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 3.9948341846466064,
      "learning_rate": 1.964466666666667e-05,
      "loss": 0.1957,
      "step": 1600
    },
    {
      "epoch": 0.07555555555555556,
      "grad_norm": 5.041555881500244,
      "learning_rate": 1.9622444444444446e-05,
      "loss": 0.1509,
      "step": 1700
    },
    {
      "epoch": 0.08,
      "grad_norm": 2.5418717861175537,
      "learning_rate": 1.9600222222222224e-05,
      "loss": 0.161,
      "step": 1800
    },
    {
      "epoch": 0.08444444444444445,
      "grad_norm": 0.14355246722698212,
      "learning_rate": 1.9578e-05,
      "loss": 0.1662,
      "step": 1900
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 2.886411428451538,
      "learning_rate": 1.955577777777778e-05,
      "loss": 0.1434,
      "step": 2000
    },
    {
      "epoch": 0.09333333333333334,
      "grad_norm": 5.221299171447754,
      "learning_rate": 1.9533555555555557e-05,
      "loss": 0.1289,
      "step": 2100
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 0.31824707984924316,
      "learning_rate": 1.9511333333333334e-05,
      "loss": 0.1615,
      "step": 2200
    },
    {
      "epoch": 0.10222222222222223,
      "grad_norm": 3.0681722164154053,
      "learning_rate": 1.9489111111111112e-05,
      "loss": 0.1524,
      "step": 2300
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.2619003355503082,
      "learning_rate": 1.946688888888889e-05,
      "loss": 0.132,
      "step": 2400
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 5.394331932067871,
      "learning_rate": 1.9444666666666667e-05,
      "loss": 0.1622,
      "step": 2500
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 2.3423843383789062,
      "learning_rate": 1.9422444444444445e-05,
      "loss": 0.1321,
      "step": 2600
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.18107706308364868,
      "learning_rate": 1.9400222222222226e-05,
      "loss": 0.1708,
      "step": 2700
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 3.350667953491211,
      "learning_rate": 1.9378e-05,
      "loss": 0.1317,
      "step": 2800
    },
    {
      "epoch": 0.1288888888888889,
      "grad_norm": 2.471285820007324,
      "learning_rate": 1.935577777777778e-05,
      "loss": 0.1351,
      "step": 2900
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.15694811940193176,
      "learning_rate": 1.9333555555555555e-05,
      "loss": 0.1447,
      "step": 3000
    },
    {
      "epoch": 0.13777777777777778,
      "grad_norm": 2.439528226852417,
      "learning_rate": 1.9311333333333336e-05,
      "loss": 0.126,
      "step": 3100
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 8.453408241271973,
      "learning_rate": 1.928911111111111e-05,
      "loss": 0.1092,
      "step": 3200
    },
    {
      "epoch": 0.14666666666666667,
      "grad_norm": 5.102726936340332,
      "learning_rate": 1.926688888888889e-05,
      "loss": 0.1595,
      "step": 3300
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 0.19630537927150726,
      "learning_rate": 1.924466666666667e-05,
      "loss": 0.1114,
      "step": 3400
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 0.21237364411354065,
      "learning_rate": 1.9222444444444446e-05,
      "loss": 0.1089,
      "step": 3500
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.1726197749376297,
      "learning_rate": 1.9200222222222224e-05,
      "loss": 0.1272,
      "step": 3600
    },
    {
      "epoch": 0.16444444444444445,
      "grad_norm": 1.150202989578247,
      "learning_rate": 1.9178000000000002e-05,
      "loss": 0.1206,
      "step": 3700
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 0.5481595396995544,
      "learning_rate": 1.915577777777778e-05,
      "loss": 0.1118,
      "step": 3800
    },
    {
      "epoch": 0.17333333333333334,
      "grad_norm": 4.101410865783691,
      "learning_rate": 1.9133555555555557e-05,
      "loss": 0.1416,
      "step": 3900
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.06618238985538483,
      "learning_rate": 1.9111333333333335e-05,
      "loss": 0.0877,
      "step": 4000
    },
    {
      "epoch": 0.18222222222222223,
      "grad_norm": 0.139478400349617,
      "learning_rate": 1.9089111111111112e-05,
      "loss": 0.1267,
      "step": 4100
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 11.389971733093262,
      "learning_rate": 1.906688888888889e-05,
      "loss": 0.1001,
      "step": 4200
    },
    {
      "epoch": 0.19111111111111112,
      "grad_norm": 7.380712032318115,
      "learning_rate": 1.9044666666666667e-05,
      "loss": 0.1269,
      "step": 4300
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 2.5321738719940186,
      "learning_rate": 1.9022444444444445e-05,
      "loss": 0.1268,
      "step": 4400
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.446819543838501,
      "learning_rate": 1.9000222222222226e-05,
      "loss": 0.1063,
      "step": 4500
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 4.013838291168213,
      "learning_rate": 1.8978e-05,
      "loss": 0.0949,
      "step": 4600
    },
    {
      "epoch": 0.2088888888888889,
      "grad_norm": 1.7025537490844727,
      "learning_rate": 1.895577777777778e-05,
      "loss": 0.1142,
      "step": 4700
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.09831699728965759,
      "learning_rate": 1.8933555555555555e-05,
      "loss": 0.1002,
      "step": 4800
    },
    {
      "epoch": 0.21777777777777776,
      "grad_norm": 0.08923840522766113,
      "learning_rate": 1.8911333333333336e-05,
      "loss": 0.1235,
      "step": 4900
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 19.144136428833008,
      "learning_rate": 1.8889111111111114e-05,
      "loss": 0.1046,
      "step": 5000
    },
    {
      "epoch": 0.22666666666666666,
      "grad_norm": 1.3510570526123047,
      "learning_rate": 1.8866888888888892e-05,
      "loss": 0.1016,
      "step": 5100
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 0.29279813170433044,
      "learning_rate": 1.884466666666667e-05,
      "loss": 0.0952,
      "step": 5200
    },
    {
      "epoch": 0.23555555555555555,
      "grad_norm": 0.1721706986427307,
      "learning_rate": 1.8822444444444447e-05,
      "loss": 0.1135,
      "step": 5300
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.5500266551971436,
      "learning_rate": 1.8800222222222225e-05,
      "loss": 0.0989,
      "step": 5400
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 0.17108505964279175,
      "learning_rate": 1.8778000000000002e-05,
      "loss": 0.1174,
      "step": 5500
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 35.16130828857422,
      "learning_rate": 1.875577777777778e-05,
      "loss": 0.096,
      "step": 5600
    },
    {
      "epoch": 0.25333333333333335,
      "grad_norm": 0.16404882073402405,
      "learning_rate": 1.8733555555555557e-05,
      "loss": 0.1129,
      "step": 5700
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 0.3715875744819641,
      "learning_rate": 1.8711333333333335e-05,
      "loss": 0.1177,
      "step": 5800
    },
    {
      "epoch": 0.26222222222222225,
      "grad_norm": 0.1689239889383316,
      "learning_rate": 1.8689111111111113e-05,
      "loss": 0.0894,
      "step": 5900
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.11431971192359924,
      "learning_rate": 1.866688888888889e-05,
      "loss": 0.0958,
      "step": 6000
    },
    {
      "epoch": 0.27111111111111114,
      "grad_norm": 0.06962915509939194,
      "learning_rate": 1.8644666666666668e-05,
      "loss": 0.0893,
      "step": 6100
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 1.7856910228729248,
      "learning_rate": 1.8622444444444445e-05,
      "loss": 0.11,
      "step": 6200
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.10107835382223129,
      "learning_rate": 1.8600222222222223e-05,
      "loss": 0.0889,
      "step": 6300
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 0.08809638023376465,
      "learning_rate": 1.8578e-05,
      "loss": 0.0919,
      "step": 6400
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 4.69072961807251,
      "learning_rate": 1.8555777777777778e-05,
      "loss": 0.1031,
      "step": 6500
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.45825883746147156,
      "learning_rate": 1.853355555555556e-05,
      "loss": 0.0769,
      "step": 6600
    },
    {
      "epoch": 0.29777777777777775,
      "grad_norm": 0.6504839062690735,
      "learning_rate": 1.8511333333333334e-05,
      "loss": 0.0937,
      "step": 6700
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 6.201122760772705,
      "learning_rate": 1.8489111111111115e-05,
      "loss": 0.0772,
      "step": 6800
    },
    {
      "epoch": 0.30666666666666664,
      "grad_norm": 0.04246969148516655,
      "learning_rate": 1.846688888888889e-05,
      "loss": 0.0693,
      "step": 6900
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 0.061337389051914215,
      "learning_rate": 1.844466666666667e-05,
      "loss": 0.0813,
      "step": 7000
    },
    {
      "epoch": 0.31555555555555553,
      "grad_norm": 0.4191155731678009,
      "learning_rate": 1.8422444444444444e-05,
      "loss": 0.0877,
      "step": 7100
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.08598490804433823,
      "learning_rate": 1.8400222222222225e-05,
      "loss": 0.102,
      "step": 7200
    },
    {
      "epoch": 0.3244444444444444,
      "grad_norm": 0.08118333667516708,
      "learning_rate": 1.8378000000000003e-05,
      "loss": 0.0734,
      "step": 7300
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 0.02676529996097088,
      "learning_rate": 1.835577777777778e-05,
      "loss": 0.0647,
      "step": 7400
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.11322008818387985,
      "learning_rate": 1.8333555555555558e-05,
      "loss": 0.1021,
      "step": 7500
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 0.053540244698524475,
      "learning_rate": 1.8311333333333335e-05,
      "loss": 0.0735,
      "step": 7600
    },
    {
      "epoch": 0.3422222222222222,
      "grad_norm": 0.1039789542555809,
      "learning_rate": 1.8289111111111113e-05,
      "loss": 0.0835,
      "step": 7700
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.07570401579141617,
      "learning_rate": 1.826688888888889e-05,
      "loss": 0.0638,
      "step": 7800
    },
    {
      "epoch": 0.3511111111111111,
      "grad_norm": 2.918757200241089,
      "learning_rate": 1.8244666666666668e-05,
      "loss": 0.0861,
      "step": 7900
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.17744995653629303,
      "learning_rate": 1.8222444444444446e-05,
      "loss": 0.0866,
      "step": 8000
    },
    {
      "epoch": 0.36,
      "grad_norm": 3.040616273880005,
      "learning_rate": 1.8200222222222224e-05,
      "loss": 0.0763,
      "step": 8100
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 0.2531903386116028,
      "learning_rate": 1.8178e-05,
      "loss": 0.095,
      "step": 8200
    },
    {
      "epoch": 0.3688888888888889,
      "grad_norm": 2.422161102294922,
      "learning_rate": 1.815577777777778e-05,
      "loss": 0.0644,
      "step": 8300
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.08520820736885071,
      "learning_rate": 1.8133555555555556e-05,
      "loss": 0.0634,
      "step": 8400
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 3.0949714183807373,
      "learning_rate": 1.8111333333333334e-05,
      "loss": 0.0835,
      "step": 8500
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 0.07553896307945251,
      "learning_rate": 1.808911111111111e-05,
      "loss": 0.0761,
      "step": 8600
    },
    {
      "epoch": 0.38666666666666666,
      "grad_norm": 0.14240621030330658,
      "learning_rate": 1.806688888888889e-05,
      "loss": 0.0765,
      "step": 8700
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 0.05360185727477074,
      "learning_rate": 1.8044666666666667e-05,
      "loss": 0.0776,
      "step": 8800
    },
    {
      "epoch": 0.39555555555555555,
      "grad_norm": 1.9490760564804077,
      "learning_rate": 1.8022444444444448e-05,
      "loss": 0.0783,
      "step": 8900
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.13656261563301086,
      "learning_rate": 1.8000222222222222e-05,
      "loss": 0.0777,
      "step": 9000
    },
    {
      "epoch": 0.40444444444444444,
      "grad_norm": 0.04413868859410286,
      "learning_rate": 1.7978000000000003e-05,
      "loss": 0.0703,
      "step": 9100
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 0.09672458469867706,
      "learning_rate": 1.7955777777777777e-05,
      "loss": 0.0674,
      "step": 9200
    },
    {
      "epoch": 0.41333333333333333,
      "grad_norm": 0.10253071039915085,
      "learning_rate": 1.7933555555555558e-05,
      "loss": 0.0825,
      "step": 9300
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 0.0567239373922348,
      "learning_rate": 1.7911333333333332e-05,
      "loss": 0.0821,
      "step": 9400
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 3.1190896034240723,
      "learning_rate": 1.7889111111111113e-05,
      "loss": 0.059,
      "step": 9500
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 5.612010478973389,
      "learning_rate": 1.786688888888889e-05,
      "loss": 0.0789,
      "step": 9600
    },
    {
      "epoch": 0.4311111111111111,
      "grad_norm": 5.662487506866455,
      "learning_rate": 1.784466666666667e-05,
      "loss": 0.0599,
      "step": 9700
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 0.08370234072208405,
      "learning_rate": 1.7822444444444446e-05,
      "loss": 0.097,
      "step": 9800
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.07630416005849838,
      "learning_rate": 1.7800222222222224e-05,
      "loss": 0.0627,
      "step": 9900
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.15356168150901794,
      "learning_rate": 1.7778e-05,
      "loss": 0.097,
      "step": 10000
    },
    {
      "epoch": 0.4488888888888889,
      "grad_norm": 0.1709073781967163,
      "learning_rate": 1.775577777777778e-05,
      "loss": 0.0727,
      "step": 10100
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.11360853165388107,
      "learning_rate": 1.7733555555555557e-05,
      "loss": 0.0552,
      "step": 10200
    },
    {
      "epoch": 0.4577777777777778,
      "grad_norm": 0.13121232390403748,
      "learning_rate": 1.7711333333333334e-05,
      "loss": 0.0827,
      "step": 10300
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 0.07282666116952896,
      "learning_rate": 1.7689111111111112e-05,
      "loss": 0.0566,
      "step": 10400
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.052061889320611954,
      "learning_rate": 1.766688888888889e-05,
      "loss": 0.0751,
      "step": 10500
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 1.9876450300216675,
      "learning_rate": 1.7644666666666667e-05,
      "loss": 0.0956,
      "step": 10600
    },
    {
      "epoch": 0.47555555555555556,
      "grad_norm": 0.18310141563415527,
      "learning_rate": 1.7622444444444445e-05,
      "loss": 0.088,
      "step": 10700
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.12073013186454773,
      "learning_rate": 1.7600222222222222e-05,
      "loss": 0.0603,
      "step": 10800
    },
    {
      "epoch": 0.48444444444444446,
      "grad_norm": 0.06265436112880707,
      "learning_rate": 1.7578e-05,
      "loss": 0.0581,
      "step": 10900
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 7.2750372886657715,
      "learning_rate": 1.7555777777777778e-05,
      "loss": 0.0656,
      "step": 11000
    },
    {
      "epoch": 0.49333333333333335,
      "grad_norm": 0.07108195126056671,
      "learning_rate": 1.7533555555555555e-05,
      "loss": 0.091,
      "step": 11100
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 0.1427740603685379,
      "learning_rate": 1.7511333333333336e-05,
      "loss": 0.0908,
      "step": 11200
    },
    {
      "epoch": 0.5022222222222222,
      "grad_norm": 0.11966036260128021,
      "learning_rate": 1.7489111111111114e-05,
      "loss": 0.0721,
      "step": 11300
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.0791458934545517,
      "learning_rate": 1.746688888888889e-05,
      "loss": 0.071,
      "step": 11400
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 0.04697681963443756,
      "learning_rate": 1.744466666666667e-05,
      "loss": 0.0521,
      "step": 11500
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 0.027715222910046577,
      "learning_rate": 1.7422444444444447e-05,
      "loss": 0.0547,
      "step": 11600
    },
    {
      "epoch": 0.52,
      "grad_norm": 0.26180535554885864,
      "learning_rate": 1.7400222222222224e-05,
      "loss": 0.0757,
      "step": 11700
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 0.12067807465791702,
      "learning_rate": 1.7378000000000002e-05,
      "loss": 0.0699,
      "step": 11800
    },
    {
      "epoch": 0.5288888888888889,
      "grad_norm": 0.07886987179517746,
      "learning_rate": 1.735577777777778e-05,
      "loss": 0.0654,
      "step": 11900
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.06717485189437866,
      "learning_rate": 1.7333555555555557e-05,
      "loss": 0.0856,
      "step": 12000
    },
    {
      "epoch": 0.5377777777777778,
      "grad_norm": 0.49019262194633484,
      "learning_rate": 1.7311333333333335e-05,
      "loss": 0.0497,
      "step": 12100
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 0.8094658851623535,
      "learning_rate": 1.7289111111111112e-05,
      "loss": 0.0479,
      "step": 12200
    },
    {
      "epoch": 0.5466666666666666,
      "grad_norm": 0.13350975513458252,
      "learning_rate": 1.726688888888889e-05,
      "loss": 0.0597,
      "step": 12300
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 1.8127812147140503,
      "learning_rate": 1.7244666666666668e-05,
      "loss": 0.0837,
      "step": 12400
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.9630727767944336,
      "learning_rate": 1.7222444444444445e-05,
      "loss": 0.0761,
      "step": 12500
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.18556270003318787,
      "learning_rate": 1.7200222222222223e-05,
      "loss": 0.0935,
      "step": 12600
    },
    {
      "epoch": 0.5644444444444444,
      "grad_norm": 0.06237618997693062,
      "learning_rate": 1.7178e-05,
      "loss": 0.068,
      "step": 12700
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 0.05005503073334694,
      "learning_rate": 1.715577777777778e-05,
      "loss": 0.0721,
      "step": 12800
    },
    {
      "epoch": 0.5733333333333334,
      "grad_norm": 0.024310402572155,
      "learning_rate": 1.7133555555555556e-05,
      "loss": 0.0763,
      "step": 12900
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 0.10856595635414124,
      "learning_rate": 1.7111333333333337e-05,
      "loss": 0.0807,
      "step": 13000
    },
    {
      "epoch": 0.5822222222222222,
      "grad_norm": 0.15524204075336456,
      "learning_rate": 1.708911111111111e-05,
      "loss": 0.0874,
      "step": 13100
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.05053148791193962,
      "learning_rate": 1.7066888888888892e-05,
      "loss": 0.0744,
      "step": 13200
    },
    {
      "epoch": 0.5911111111111111,
      "grad_norm": 0.10311796516180038,
      "learning_rate": 1.7044666666666666e-05,
      "loss": 0.0872,
      "step": 13300
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 11.86903190612793,
      "learning_rate": 1.7022444444444447e-05,
      "loss": 0.0832,
      "step": 13400
    },
    {
      "epoch": 0.6,
      "grad_norm": 5.191854476928711,
      "learning_rate": 1.7000222222222225e-05,
      "loss": 0.0593,
      "step": 13500
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 0.06243111193180084,
      "learning_rate": 1.6978000000000002e-05,
      "loss": 0.0739,
      "step": 13600
    },
    {
      "epoch": 0.6088888888888889,
      "grad_norm": 0.3291846811771393,
      "learning_rate": 1.695577777777778e-05,
      "loss": 0.076,
      "step": 13700
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.08195248246192932,
      "learning_rate": 1.6933555555555558e-05,
      "loss": 0.0616,
      "step": 13800
    },
    {
      "epoch": 0.6177777777777778,
      "grad_norm": 1.422676920890808,
      "learning_rate": 1.6911333333333335e-05,
      "loss": 0.0792,
      "step": 13900
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.048084307461977005,
      "learning_rate": 1.6889111111111113e-05,
      "loss": 0.0757,
      "step": 14000
    },
    {
      "epoch": 0.6266666666666667,
      "grad_norm": 0.3529258072376251,
      "learning_rate": 1.686688888888889e-05,
      "loss": 0.0824,
      "step": 14100
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 0.07232304662466049,
      "learning_rate": 1.6844666666666668e-05,
      "loss": 0.0887,
      "step": 14200
    },
    {
      "epoch": 0.6355555555555555,
      "grad_norm": 1.856327772140503,
      "learning_rate": 1.6822444444444446e-05,
      "loss": 0.0529,
      "step": 14300
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.08177308738231659,
      "learning_rate": 1.6800222222222223e-05,
      "loss": 0.0702,
      "step": 14400
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 0.11269660294055939,
      "learning_rate": 1.6778e-05,
      "loss": 0.0717,
      "step": 14500
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 0.06502548605203629,
      "learning_rate": 1.675577777777778e-05,
      "loss": 0.0515,
      "step": 14600
    },
    {
      "epoch": 0.6533333333333333,
      "grad_norm": 0.11075831204652786,
      "learning_rate": 1.6733555555555556e-05,
      "loss": 0.0782,
      "step": 14700
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 26.862098693847656,
      "learning_rate": 1.6711333333333334e-05,
      "loss": 0.0416,
      "step": 14800
    },
    {
      "epoch": 0.6622222222222223,
      "grad_norm": 0.17313483357429504,
      "learning_rate": 1.668911111111111e-05,
      "loss": 0.0549,
      "step": 14900
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 2.014615535736084,
      "learning_rate": 1.666688888888889e-05,
      "loss": 0.0686,
      "step": 15000
    },
    {
      "epoch": 0.6711111111111111,
      "grad_norm": 1.896836280822754,
      "learning_rate": 1.664466666666667e-05,
      "loss": 0.0625,
      "step": 15100
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 0.059741515666246414,
      "learning_rate": 1.6622444444444444e-05,
      "loss": 0.0424,
      "step": 15200
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.3155185282230377,
      "learning_rate": 1.6600222222222225e-05,
      "loss": 0.0826,
      "step": 15300
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 0.043150968849658966,
      "learning_rate": 1.6578e-05,
      "loss": 0.0624,
      "step": 15400
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 0.033249687403440475,
      "learning_rate": 1.655577777777778e-05,
      "loss": 0.0561,
      "step": 15500
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.021676966920495033,
      "learning_rate": 1.6533555555555555e-05,
      "loss": 0.0475,
      "step": 15600
    },
    {
      "epoch": 0.6977777777777778,
      "grad_norm": 7.150242328643799,
      "learning_rate": 1.6511333333333336e-05,
      "loss": 0.0663,
      "step": 15700
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 1.7733651399612427,
      "learning_rate": 1.6489111111111113e-05,
      "loss": 0.0513,
      "step": 15800
    },
    {
      "epoch": 0.7066666666666667,
      "grad_norm": 0.06297924369573593,
      "learning_rate": 1.646688888888889e-05,
      "loss": 0.0432,
      "step": 15900
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.450767457485199,
      "learning_rate": 1.644466666666667e-05,
      "loss": 0.0626,
      "step": 16000
    },
    {
      "epoch": 0.7155555555555555,
      "grad_norm": 0.14853429794311523,
      "learning_rate": 1.6422444444444446e-05,
      "loss": 0.0654,
      "step": 16100
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.11139940470457077,
      "learning_rate": 1.6400222222222224e-05,
      "loss": 0.0563,
      "step": 16200
    },
    {
      "epoch": 0.7244444444444444,
      "grad_norm": 0.09810448437929153,
      "learning_rate": 1.6378e-05,
      "loss": 0.085,
      "step": 16300
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 0.5836544036865234,
      "learning_rate": 1.635577777777778e-05,
      "loss": 0.0486,
      "step": 16400
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 0.03643977642059326,
      "learning_rate": 1.6333555555555557e-05,
      "loss": 0.0557,
      "step": 16500
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 0.21759556233882904,
      "learning_rate": 1.6311333333333334e-05,
      "loss": 0.0853,
      "step": 16600
    },
    {
      "epoch": 0.7422222222222222,
      "grad_norm": 0.02789228968322277,
      "learning_rate": 1.6289111111111112e-05,
      "loss": 0.0375,
      "step": 16700
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.09411098808050156,
      "learning_rate": 1.626688888888889e-05,
      "loss": 0.0519,
      "step": 16800
    },
    {
      "epoch": 0.7511111111111111,
      "grad_norm": 0.16459380090236664,
      "learning_rate": 1.6244666666666667e-05,
      "loss": 0.0389,
      "step": 16900
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 3.891287326812744,
      "learning_rate": 1.6222444444444445e-05,
      "loss": 0.0236,
      "step": 17000
    },
    {
      "epoch": 0.76,
      "grad_norm": 0.023060929030179977,
      "learning_rate": 1.6200222222222222e-05,
      "loss": 0.0509,
      "step": 17100
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 0.034144237637519836,
      "learning_rate": 1.6178e-05,
      "loss": 0.0438,
      "step": 17200
    },
    {
      "epoch": 0.7688888888888888,
      "grad_norm": 0.02957378514111042,
      "learning_rate": 1.6155777777777778e-05,
      "loss": 0.0417,
      "step": 17300
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 19.23971939086914,
      "learning_rate": 1.613355555555556e-05,
      "loss": 0.0533,
      "step": 17400
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.059362128376960754,
      "learning_rate": 1.6111333333333333e-05,
      "loss": 0.0458,
      "step": 17500
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 0.07794655114412308,
      "learning_rate": 1.6089111111111114e-05,
      "loss": 0.0458,
      "step": 17600
    },
    {
      "epoch": 0.7866666666666666,
      "grad_norm": 0.14612358808517456,
      "learning_rate": 1.6066888888888888e-05,
      "loss": 0.0753,
      "step": 17700
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 0.45540833473205566,
      "learning_rate": 1.604466666666667e-05,
      "loss": 0.0603,
      "step": 17800
    },
    {
      "epoch": 0.7955555555555556,
      "grad_norm": 0.3341798484325409,
      "learning_rate": 1.6022444444444443e-05,
      "loss": 0.0641,
      "step": 17900
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.0888703390955925,
      "learning_rate": 1.6000222222222224e-05,
      "loss": 0.0547,
      "step": 18000
    },
    {
      "epoch": 0.8044444444444444,
      "grad_norm": 0.17061907052993774,
      "learning_rate": 1.5978000000000002e-05,
      "loss": 0.0391,
      "step": 18100
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 0.028862448409199715,
      "learning_rate": 1.595577777777778e-05,
      "loss": 0.0404,
      "step": 18200
    },
    {
      "epoch": 0.8133333333333334,
      "grad_norm": 0.07775436341762543,
      "learning_rate": 1.5933555555555557e-05,
      "loss": 0.0532,
      "step": 18300
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 0.04832173511385918,
      "learning_rate": 1.5911333333333335e-05,
      "loss": 0.06,
      "step": 18400
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 0.06448986381292343,
      "learning_rate": 1.5889111111111112e-05,
      "loss": 0.0528,
      "step": 18500
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.056927040219306946,
      "learning_rate": 1.586688888888889e-05,
      "loss": 0.0514,
      "step": 18600
    },
    {
      "epoch": 0.8311111111111111,
      "grad_norm": 0.18941116333007812,
      "learning_rate": 1.5844666666666668e-05,
      "loss": 0.0514,
      "step": 18700
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 0.2508240044116974,
      "learning_rate": 1.5822444444444445e-05,
      "loss": 0.0503,
      "step": 18800
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.5942667126655579,
      "learning_rate": 1.5800222222222223e-05,
      "loss": 0.0475,
      "step": 18900
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.07824631780385971,
      "learning_rate": 1.5778000000000004e-05,
      "loss": 0.0487,
      "step": 19000
    },
    {
      "epoch": 0.8488888888888889,
      "grad_norm": 0.057507582008838654,
      "learning_rate": 1.5755777777777778e-05,
      "loss": 0.0414,
      "step": 19100
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.03539557754993439,
      "learning_rate": 1.573355555555556e-05,
      "loss": 0.052,
      "step": 19200
    },
    {
      "epoch": 0.8577777777777778,
      "grad_norm": 1.906468152999878,
      "learning_rate": 1.5711333333333333e-05,
      "loss": 0.0756,
      "step": 19300
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 0.07410060614347458,
      "learning_rate": 1.5689111111111114e-05,
      "loss": 0.0449,
      "step": 19400
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 0.7052868008613586,
      "learning_rate": 1.566688888888889e-05,
      "loss": 0.0707,
      "step": 19500
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 11.438714981079102,
      "learning_rate": 1.564466666666667e-05,
      "loss": 0.074,
      "step": 19600
    },
    {
      "epoch": 0.8755555555555555,
      "grad_norm": 0.10097905993461609,
      "learning_rate": 1.5622444444444447e-05,
      "loss": 0.0639,
      "step": 19700
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.989078938961029,
      "learning_rate": 1.5600222222222225e-05,
      "loss": 0.0411,
      "step": 19800
    },
    {
      "epoch": 0.8844444444444445,
      "grad_norm": 0.03249847888946533,
      "learning_rate": 1.5578000000000002e-05,
      "loss": 0.0689,
      "step": 19900
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.06093044579029083,
      "learning_rate": 1.555577777777778e-05,
      "loss": 0.0531,
      "step": 20000
    },
    {
      "epoch": 0.8933333333333333,
      "grad_norm": 0.08135195076465607,
      "learning_rate": 1.5533555555555558e-05,
      "loss": 0.0488,
      "step": 20100
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 0.03431054577231407,
      "learning_rate": 1.5511333333333335e-05,
      "loss": 0.0372,
      "step": 20200
    },
    {
      "epoch": 0.9022222222222223,
      "grad_norm": 4.826496124267578,
      "learning_rate": 1.5489111111111113e-05,
      "loss": 0.0763,
      "step": 20300
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.05180704966187477,
      "learning_rate": 1.546688888888889e-05,
      "loss": 0.0709,
      "step": 20400
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 0.04970724880695343,
      "learning_rate": 1.5444666666666668e-05,
      "loss": 0.0373,
      "step": 20500
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 0.03228358179330826,
      "learning_rate": 1.5422444444444446e-05,
      "loss": 0.0415,
      "step": 20600
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.1231883317232132,
      "learning_rate": 1.5400222222222223e-05,
      "loss": 0.0601,
      "step": 20700
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 0.044086214154958725,
      "learning_rate": 1.5378e-05,
      "loss": 0.0367,
      "step": 20800
    },
    {
      "epoch": 0.9288888888888889,
      "grad_norm": 0.07467462122440338,
      "learning_rate": 1.535577777777778e-05,
      "loss": 0.0488,
      "step": 20900
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.025288356468081474,
      "learning_rate": 1.5333555555555556e-05,
      "loss": 0.0326,
      "step": 21000
    },
    {
      "epoch": 0.9377777777777778,
      "grad_norm": 75.5598373413086,
      "learning_rate": 1.5311333333333334e-05,
      "loss": 0.0541,
      "step": 21100
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 0.03404108062386513,
      "learning_rate": 1.528911111111111e-05,
      "loss": 0.0378,
      "step": 21200
    },
    {
      "epoch": 0.9466666666666667,
      "grad_norm": 6.406930446624756,
      "learning_rate": 1.5266888888888892e-05,
      "loss": 0.0487,
      "step": 21300
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 0.052216239273548126,
      "learning_rate": 1.5244666666666668e-05,
      "loss": 0.038,
      "step": 21400
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 0.04824044927954674,
      "learning_rate": 1.5222444444444446e-05,
      "loss": 0.0502,
      "step": 21500
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.03507446125149727,
      "learning_rate": 1.5200222222222223e-05,
      "loss": 0.0676,
      "step": 21600
    },
    {
      "epoch": 0.9644444444444444,
      "grad_norm": 1.9862031936645508,
      "learning_rate": 1.5178000000000001e-05,
      "loss": 0.0454,
      "step": 21700
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 2.0376853942871094,
      "learning_rate": 1.5155777777777779e-05,
      "loss": 0.052,
      "step": 21800
    },
    {
      "epoch": 0.9733333333333334,
      "grad_norm": 0.15395233035087585,
      "learning_rate": 1.5133555555555556e-05,
      "loss": 0.0501,
      "step": 21900
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.023577861487865448,
      "learning_rate": 1.5111333333333336e-05,
      "loss": 0.038,
      "step": 22000
    },
    {
      "epoch": 0.9822222222222222,
      "grad_norm": 0.04443955048918724,
      "learning_rate": 1.5089111111111112e-05,
      "loss": 0.045,
      "step": 22100
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.035848408937454224,
      "learning_rate": 1.506688888888889e-05,
      "loss": 0.0467,
      "step": 22200
    },
    {
      "epoch": 0.9911111111111112,
      "grad_norm": 0.6771757006645203,
      "learning_rate": 1.5044666666666667e-05,
      "loss": 0.0334,
      "step": 22300
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 0.056160226464271545,
      "learning_rate": 1.5022444444444446e-05,
      "loss": 0.0482,
      "step": 22400
    },
    {
      "epoch": 1.0,
      "grad_norm": 15.614927291870117,
      "learning_rate": 1.5000222222222222e-05,
      "loss": 0.0465,
      "step": 22500
    },
    {
      "epoch": 1.0044444444444445,
      "grad_norm": 0.4398789405822754,
      "learning_rate": 1.4978000000000001e-05,
      "loss": 0.0449,
      "step": 22600
    },
    {
      "epoch": 1.008888888888889,
      "grad_norm": 0.057956207543611526,
      "learning_rate": 1.4955777777777777e-05,
      "loss": 0.0296,
      "step": 22700
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.04388073459267616,
      "learning_rate": 1.4933555555555556e-05,
      "loss": 0.036,
      "step": 22800
    },
    {
      "epoch": 1.0177777777777777,
      "grad_norm": 0.02277284488081932,
      "learning_rate": 1.4911333333333336e-05,
      "loss": 0.0175,
      "step": 22900
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 0.2212652713060379,
      "learning_rate": 1.4889111111111112e-05,
      "loss": 0.049,
      "step": 23000
    },
    {
      "epoch": 1.0266666666666666,
      "grad_norm": 0.011224715039134026,
      "learning_rate": 1.4866888888888891e-05,
      "loss": 0.0219,
      "step": 23100
    },
    {
      "epoch": 1.031111111111111,
      "grad_norm": 0.03565581515431404,
      "learning_rate": 1.4844666666666667e-05,
      "loss": 0.062,
      "step": 23200
    },
    {
      "epoch": 1.0355555555555556,
      "grad_norm": 1.7860397100448608,
      "learning_rate": 1.4822444444444446e-05,
      "loss": 0.0449,
      "step": 23300
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.044911760836839676,
      "learning_rate": 1.4800222222222222e-05,
      "loss": 0.0226,
      "step": 23400
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 0.013913743197917938,
      "learning_rate": 1.4778000000000001e-05,
      "loss": 0.0225,
      "step": 23500
    },
    {
      "epoch": 1.048888888888889,
      "grad_norm": 42.531341552734375,
      "learning_rate": 1.475577777777778e-05,
      "loss": 0.0283,
      "step": 23600
    },
    {
      "epoch": 1.0533333333333332,
      "grad_norm": 0.08428775519132614,
      "learning_rate": 1.4733555555555557e-05,
      "loss": 0.0234,
      "step": 23700
    },
    {
      "epoch": 1.0577777777777777,
      "grad_norm": 0.10971103608608246,
      "learning_rate": 1.4711333333333336e-05,
      "loss": 0.0439,
      "step": 23800
    },
    {
      "epoch": 1.0622222222222222,
      "grad_norm": 0.023833446204662323,
      "learning_rate": 1.4689111111111112e-05,
      "loss": 0.0427,
      "step": 23900
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.033556561917066574,
      "learning_rate": 1.4666888888888891e-05,
      "loss": 0.0293,
      "step": 24000
    },
    {
      "epoch": 1.0711111111111111,
      "grad_norm": 0.03156523406505585,
      "learning_rate": 1.4644666666666667e-05,
      "loss": 0.028,
      "step": 24100
    },
    {
      "epoch": 1.0755555555555556,
      "grad_norm": 0.03372789919376373,
      "learning_rate": 1.4622444444444446e-05,
      "loss": 0.0258,
      "step": 24200
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.03753506764769554,
      "learning_rate": 1.4600222222222224e-05,
      "loss": 0.0378,
      "step": 24300
    },
    {
      "epoch": 1.0844444444444445,
      "grad_norm": 0.03637111559510231,
      "learning_rate": 1.4578000000000002e-05,
      "loss": 0.054,
      "step": 24400
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.03606487810611725,
      "learning_rate": 1.455577777777778e-05,
      "loss": 0.0446,
      "step": 24500
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.039939217269420624,
      "learning_rate": 1.4533555555555557e-05,
      "loss": 0.0414,
      "step": 24600
    },
    {
      "epoch": 1.0977777777777777,
      "grad_norm": 0.08733846247196198,
      "learning_rate": 1.4511333333333335e-05,
      "loss": 0.0555,
      "step": 24700
    },
    {
      "epoch": 1.1022222222222222,
      "grad_norm": 0.03232159465551376,
      "learning_rate": 1.4489111111111112e-05,
      "loss": 0.0337,
      "step": 24800
    },
    {
      "epoch": 1.1066666666666667,
      "grad_norm": 0.2041034996509552,
      "learning_rate": 1.446688888888889e-05,
      "loss": 0.0211,
      "step": 24900
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.0433790497481823,
      "learning_rate": 1.4444666666666667e-05,
      "loss": 0.0469,
      "step": 25000
    },
    {
      "epoch": 1.1155555555555556,
      "grad_norm": 0.1052144318819046,
      "learning_rate": 1.4422444444444445e-05,
      "loss": 0.0256,
      "step": 25100
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.012112833559513092,
      "learning_rate": 1.4400222222222224e-05,
      "loss": 0.0254,
      "step": 25200
    },
    {
      "epoch": 1.1244444444444444,
      "grad_norm": 0.24665884673595428,
      "learning_rate": 1.4378e-05,
      "loss": 0.033,
      "step": 25300
    },
    {
      "epoch": 1.1288888888888888,
      "grad_norm": 0.03128701075911522,
      "learning_rate": 1.435577777777778e-05,
      "loss": 0.0357,
      "step": 25400
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 0.03856588527560234,
      "learning_rate": 1.4333555555555555e-05,
      "loss": 0.0307,
      "step": 25500
    },
    {
      "epoch": 1.1377777777777778,
      "grad_norm": 0.21967433393001556,
      "learning_rate": 1.4311333333333335e-05,
      "loss": 0.0358,
      "step": 25600
    },
    {
      "epoch": 1.1422222222222222,
      "grad_norm": 0.049971096217632294,
      "learning_rate": 1.428911111111111e-05,
      "loss": 0.0515,
      "step": 25700
    },
    {
      "epoch": 1.1466666666666667,
      "grad_norm": 0.16927500069141388,
      "learning_rate": 1.426688888888889e-05,
      "loss": 0.0363,
      "step": 25800
    },
    {
      "epoch": 1.1511111111111112,
      "grad_norm": 0.027340544387698174,
      "learning_rate": 1.424466666666667e-05,
      "loss": 0.0281,
      "step": 25900
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 2.592930316925049,
      "learning_rate": 1.4222444444444445e-05,
      "loss": 0.0391,
      "step": 26000
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.12336016446352005,
      "learning_rate": 1.4200222222222225e-05,
      "loss": 0.0384,
      "step": 26100
    },
    {
      "epoch": 1.1644444444444444,
      "grad_norm": 0.1318233758211136,
      "learning_rate": 1.4178e-05,
      "loss": 0.0206,
      "step": 26200
    },
    {
      "epoch": 1.1688888888888889,
      "grad_norm": 0.01496175117790699,
      "learning_rate": 1.415577777777778e-05,
      "loss": 0.0288,
      "step": 26300
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 0.05535997450351715,
      "learning_rate": 1.4133555555555556e-05,
      "loss": 0.034,
      "step": 26400
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.023591332137584686,
      "learning_rate": 1.4111333333333335e-05,
      "loss": 0.0396,
      "step": 26500
    },
    {
      "epoch": 1.1822222222222223,
      "grad_norm": 0.014231081120669842,
      "learning_rate": 1.4089111111111111e-05,
      "loss": 0.016,
      "step": 26600
    },
    {
      "epoch": 1.1866666666666668,
      "grad_norm": 0.02683933451771736,
      "learning_rate": 1.406688888888889e-05,
      "loss": 0.0387,
      "step": 26700
    },
    {
      "epoch": 1.1911111111111112,
      "grad_norm": 0.13557679951190948,
      "learning_rate": 1.4044666666666668e-05,
      "loss": 0.0304,
      "step": 26800
    },
    {
      "epoch": 1.1955555555555555,
      "grad_norm": 0.02806561440229416,
      "learning_rate": 1.4022444444444445e-05,
      "loss": 0.0367,
      "step": 26900
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.0691317766904831,
      "learning_rate": 1.4000222222222225e-05,
      "loss": 0.0381,
      "step": 27000
    },
    {
      "epoch": 1.2044444444444444,
      "grad_norm": 0.3027394711971283,
      "learning_rate": 1.3978e-05,
      "loss": 0.0391,
      "step": 27100
    },
    {
      "epoch": 1.208888888888889,
      "grad_norm": 0.03266189247369766,
      "learning_rate": 1.395577777777778e-05,
      "loss": 0.0204,
      "step": 27200
    },
    {
      "epoch": 1.2133333333333334,
      "grad_norm": 0.03701596334576607,
      "learning_rate": 1.3933555555555556e-05,
      "loss": 0.0243,
      "step": 27300
    },
    {
      "epoch": 1.2177777777777778,
      "grad_norm": 0.1229495257139206,
      "learning_rate": 1.3911333333333335e-05,
      "loss": 0.0309,
      "step": 27400
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.025382254272699356,
      "learning_rate": 1.3889111111111113e-05,
      "loss": 0.0358,
      "step": 27500
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 0.24272188544273376,
      "learning_rate": 1.386688888888889e-05,
      "loss": 0.0473,
      "step": 27600
    },
    {
      "epoch": 1.231111111111111,
      "grad_norm": 0.027834758162498474,
      "learning_rate": 1.3844666666666668e-05,
      "loss": 0.0247,
      "step": 27700
    },
    {
      "epoch": 1.2355555555555555,
      "grad_norm": 0.016349835321307182,
      "learning_rate": 1.3822444444444446e-05,
      "loss": 0.0265,
      "step": 27800
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.01887553744018078,
      "learning_rate": 1.3800222222222223e-05,
      "loss": 0.0202,
      "step": 27900
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 0.04669317603111267,
      "learning_rate": 1.3778000000000001e-05,
      "loss": 0.0408,
      "step": 28000
    },
    {
      "epoch": 1.248888888888889,
      "grad_norm": 3.0880179405212402,
      "learning_rate": 1.3755777777777779e-05,
      "loss": 0.0327,
      "step": 28100
    },
    {
      "epoch": 1.2533333333333334,
      "grad_norm": 0.03148694708943367,
      "learning_rate": 1.3733555555555558e-05,
      "loss": 0.0376,
      "step": 28200
    },
    {
      "epoch": 1.2577777777777777,
      "grad_norm": 0.02810570038855076,
      "learning_rate": 1.3711333333333334e-05,
      "loss": 0.0484,
      "step": 28300
    },
    {
      "epoch": 1.2622222222222224,
      "grad_norm": 4.004777431488037,
      "learning_rate": 1.3689111111111113e-05,
      "loss": 0.0188,
      "step": 28400
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.01514962874352932,
      "learning_rate": 1.3666888888888889e-05,
      "loss": 0.0305,
      "step": 28500
    },
    {
      "epoch": 1.271111111111111,
      "grad_norm": 0.038688577711582184,
      "learning_rate": 1.3644666666666668e-05,
      "loss": 0.0218,
      "step": 28600
    },
    {
      "epoch": 1.2755555555555556,
      "grad_norm": 0.042216524481773376,
      "learning_rate": 1.3622444444444444e-05,
      "loss": 0.0313,
      "step": 28700
    },
    {
      "epoch": 1.28,
      "grad_norm": 3.9622080326080322,
      "learning_rate": 1.3600222222222224e-05,
      "loss": 0.041,
      "step": 28800
    },
    {
      "epoch": 1.2844444444444445,
      "grad_norm": 2.5616767406463623,
      "learning_rate": 1.3578e-05,
      "loss": 0.0397,
      "step": 28900
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 0.053153347223997116,
      "learning_rate": 1.3555777777777779e-05,
      "loss": 0.022,
      "step": 29000
    },
    {
      "epoch": 1.2933333333333334,
      "grad_norm": 0.008972886949777603,
      "learning_rate": 1.3533555555555558e-05,
      "loss": 0.0318,
      "step": 29100
    },
    {
      "epoch": 1.2977777777777777,
      "grad_norm": 0.02333524264395237,
      "learning_rate": 1.3511333333333334e-05,
      "loss": 0.0229,
      "step": 29200
    },
    {
      "epoch": 1.3022222222222222,
      "grad_norm": 1.9329898357391357,
      "learning_rate": 1.3489111111111113e-05,
      "loss": 0.0251,
      "step": 29300
    },
    {
      "epoch": 1.3066666666666666,
      "grad_norm": 0.0217032078653574,
      "learning_rate": 1.346688888888889e-05,
      "loss": 0.0162,
      "step": 29400
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 0.04401608556509018,
      "learning_rate": 1.3444666666666668e-05,
      "loss": 0.066,
      "step": 29500
    },
    {
      "epoch": 1.3155555555555556,
      "grad_norm": 3.2659051418304443,
      "learning_rate": 1.3422444444444444e-05,
      "loss": 0.0375,
      "step": 29600
    },
    {
      "epoch": 1.32,
      "grad_norm": 2.037550210952759,
      "learning_rate": 1.3400222222222224e-05,
      "loss": 0.029,
      "step": 29700
    },
    {
      "epoch": 1.3244444444444445,
      "grad_norm": 0.3056609034538269,
      "learning_rate": 1.3378000000000001e-05,
      "loss": 0.0314,
      "step": 29800
    },
    {
      "epoch": 1.3288888888888888,
      "grad_norm": 0.15743032097816467,
      "learning_rate": 1.3355777777777779e-05,
      "loss": 0.0196,
      "step": 29900
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.028728405013680458,
      "learning_rate": 1.3333555555555557e-05,
      "loss": 0.0269,
      "step": 30000
    },
    {
      "epoch": 1.3377777777777777,
      "grad_norm": 0.01886565238237381,
      "learning_rate": 1.3311333333333334e-05,
      "loss": 0.0156,
      "step": 30100
    },
    {
      "epoch": 1.3422222222222222,
      "grad_norm": 5.232702255249023,
      "learning_rate": 1.3289111111111112e-05,
      "loss": 0.0403,
      "step": 30200
    },
    {
      "epoch": 1.3466666666666667,
      "grad_norm": 0.021387232467532158,
      "learning_rate": 1.326688888888889e-05,
      "loss": 0.032,
      "step": 30300
    },
    {
      "epoch": 1.3511111111111112,
      "grad_norm": 0.022503014653921127,
      "learning_rate": 1.3244666666666669e-05,
      "loss": 0.0406,
      "step": 30400
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.017128681764006615,
      "learning_rate": 1.3222444444444445e-05,
      "loss": 0.0282,
      "step": 30500
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.017657829448580742,
      "learning_rate": 1.3200222222222224e-05,
      "loss": 0.0249,
      "step": 30600
    },
    {
      "epoch": 1.3644444444444446,
      "grad_norm": 5.77452278137207,
      "learning_rate": 1.3178000000000002e-05,
      "loss": 0.0216,
      "step": 30700
    },
    {
      "epoch": 1.3688888888888888,
      "grad_norm": 0.037559423595666885,
      "learning_rate": 1.315577777777778e-05,
      "loss": 0.0324,
      "step": 30800
    },
    {
      "epoch": 1.3733333333333333,
      "grad_norm": 0.024956727400422096,
      "learning_rate": 1.3133555555555557e-05,
      "loss": 0.0282,
      "step": 30900
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 0.020846987143158913,
      "learning_rate": 1.3111333333333334e-05,
      "loss": 0.0381,
      "step": 31000
    },
    {
      "epoch": 1.3822222222222222,
      "grad_norm": 0.014993836171925068,
      "learning_rate": 1.3089111111111112e-05,
      "loss": 0.0158,
      "step": 31100
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 0.03585866093635559,
      "learning_rate": 1.306688888888889e-05,
      "loss": 0.0498,
      "step": 31200
    },
    {
      "epoch": 1.3911111111111112,
      "grad_norm": 0.8786661028862,
      "learning_rate": 1.3044666666666667e-05,
      "loss": 0.04,
      "step": 31300
    },
    {
      "epoch": 1.3955555555555557,
      "grad_norm": 0.03437859192490578,
      "learning_rate": 1.3022444444444447e-05,
      "loss": 0.0205,
      "step": 31400
    },
    {
      "epoch": 1.4,
      "grad_norm": 3.682541608810425,
      "learning_rate": 1.3000222222222222e-05,
      "loss": 0.0284,
      "step": 31500
    },
    {
      "epoch": 1.4044444444444444,
      "grad_norm": 2.536203384399414,
      "learning_rate": 1.2978000000000002e-05,
      "loss": 0.0353,
      "step": 31600
    },
    {
      "epoch": 1.4088888888888889,
      "grad_norm": 0.053311076015233994,
      "learning_rate": 1.2955777777777778e-05,
      "loss": 0.0279,
      "step": 31700
    },
    {
      "epoch": 1.4133333333333333,
      "grad_norm": 2.147507905960083,
      "learning_rate": 1.2933555555555557e-05,
      "loss": 0.0437,
      "step": 31800
    },
    {
      "epoch": 1.4177777777777778,
      "grad_norm": 0.06871849298477173,
      "learning_rate": 1.2911333333333333e-05,
      "loss": 0.0429,
      "step": 31900
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 0.008204569108784199,
      "learning_rate": 1.2889111111111112e-05,
      "loss": 0.0161,
      "step": 32000
    },
    {
      "epoch": 1.4266666666666667,
      "grad_norm": 0.010817531496286392,
      "learning_rate": 1.2866888888888892e-05,
      "loss": 0.015,
      "step": 32100
    },
    {
      "epoch": 1.431111111111111,
      "grad_norm": 0.03266347572207451,
      "learning_rate": 1.2844666666666667e-05,
      "loss": 0.0357,
      "step": 32200
    },
    {
      "epoch": 1.4355555555555555,
      "grad_norm": 0.012216325849294662,
      "learning_rate": 1.2822444444444447e-05,
      "loss": 0.0463,
      "step": 32300
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.04244761914014816,
      "learning_rate": 1.2800222222222223e-05,
      "loss": 0.0277,
      "step": 32400
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.014845124445855618,
      "learning_rate": 1.2778000000000002e-05,
      "loss": 0.0301,
      "step": 32500
    },
    {
      "epoch": 1.448888888888889,
      "grad_norm": 0.058134764432907104,
      "learning_rate": 1.2755777777777778e-05,
      "loss": 0.0494,
      "step": 32600
    },
    {
      "epoch": 1.4533333333333334,
      "grad_norm": 3.626774311065674,
      "learning_rate": 1.2733555555555557e-05,
      "loss": 0.0234,
      "step": 32700
    },
    {
      "epoch": 1.4577777777777778,
      "grad_norm": 0.3064216673374176,
      "learning_rate": 1.2711333333333333e-05,
      "loss": 0.0229,
      "step": 32800
    },
    {
      "epoch": 1.462222222222222,
      "grad_norm": 0.023227619007229805,
      "learning_rate": 1.2689111111111112e-05,
      "loss": 0.0113,
      "step": 32900
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 0.009010943584144115,
      "learning_rate": 1.266688888888889e-05,
      "loss": 0.016,
      "step": 33000
    },
    {
      "epoch": 1.471111111111111,
      "grad_norm": 0.02820013277232647,
      "learning_rate": 1.2644666666666668e-05,
      "loss": 0.0475,
      "step": 33100
    },
    {
      "epoch": 1.4755555555555555,
      "grad_norm": 0.028188979253172874,
      "learning_rate": 1.2622444444444445e-05,
      "loss": 0.0302,
      "step": 33200
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.017294829711318016,
      "learning_rate": 1.2600222222222223e-05,
      "loss": 0.0184,
      "step": 33300
    },
    {
      "epoch": 1.4844444444444445,
      "grad_norm": 0.03275224566459656,
      "learning_rate": 1.2578e-05,
      "loss": 0.0464,
      "step": 33400
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 0.00915344338864088,
      "learning_rate": 1.2555777777777778e-05,
      "loss": 0.0353,
      "step": 33500
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 34.73749923706055,
      "learning_rate": 1.2533555555555556e-05,
      "loss": 0.0289,
      "step": 33600
    },
    {
      "epoch": 1.4977777777777779,
      "grad_norm": 0.05392719805240631,
      "learning_rate": 1.2511333333333335e-05,
      "loss": 0.0386,
      "step": 33700
    },
    {
      "epoch": 1.5022222222222221,
      "grad_norm": 0.015013695694506168,
      "learning_rate": 1.2489111111111113e-05,
      "loss": 0.0315,
      "step": 33800
    },
    {
      "epoch": 1.5066666666666668,
      "grad_norm": 0.021999169141054153,
      "learning_rate": 1.246688888888889e-05,
      "loss": 0.0343,
      "step": 33900
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 0.026112917810678482,
      "learning_rate": 1.2444666666666668e-05,
      "loss": 0.0418,
      "step": 34000
    },
    {
      "epoch": 1.5155555555555555,
      "grad_norm": 0.02534698136150837,
      "learning_rate": 1.2422444444444446e-05,
      "loss": 0.0292,
      "step": 34100
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.15231865644454956,
      "learning_rate": 1.2400222222222223e-05,
      "loss": 0.0203,
      "step": 34200
    },
    {
      "epoch": 1.5244444444444445,
      "grad_norm": 0.018377529457211494,
      "learning_rate": 1.2378e-05,
      "loss": 0.0341,
      "step": 34300
    },
    {
      "epoch": 1.528888888888889,
      "grad_norm": 0.011889574117958546,
      "learning_rate": 1.235577777777778e-05,
      "loss": 0.0257,
      "step": 34400
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.014873831532895565,
      "learning_rate": 1.2333555555555556e-05,
      "loss": 0.0321,
      "step": 34500
    },
    {
      "epoch": 1.537777777777778,
      "grad_norm": 0.01135020051151514,
      "learning_rate": 1.2311333333333335e-05,
      "loss": 0.0247,
      "step": 34600
    },
    {
      "epoch": 1.5422222222222222,
      "grad_norm": 0.07396241277456284,
      "learning_rate": 1.2289111111111111e-05,
      "loss": 0.0409,
      "step": 34700
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 0.027225535362958908,
      "learning_rate": 1.226688888888889e-05,
      "loss": 0.0335,
      "step": 34800
    },
    {
      "epoch": 1.551111111111111,
      "grad_norm": 0.017699291929602623,
      "learning_rate": 1.2244666666666666e-05,
      "loss": 0.0178,
      "step": 34900
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 0.021373949944972992,
      "learning_rate": 1.2222444444444446e-05,
      "loss": 0.0375,
      "step": 35000
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.01383987721055746,
      "learning_rate": 1.2200222222222222e-05,
      "loss": 0.0189,
      "step": 35100
    },
    {
      "epoch": 1.5644444444444443,
      "grad_norm": 0.07020499557256699,
      "learning_rate": 1.2178000000000001e-05,
      "loss": 0.0165,
      "step": 35200
    },
    {
      "epoch": 1.568888888888889,
      "grad_norm": 0.10864581167697906,
      "learning_rate": 1.215577777777778e-05,
      "loss": 0.0313,
      "step": 35300
    },
    {
      "epoch": 1.5733333333333333,
      "grad_norm": 0.021045910194516182,
      "learning_rate": 1.2133555555555556e-05,
      "loss": 0.0096,
      "step": 35400
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 0.04443470016121864,
      "learning_rate": 1.2111333333333336e-05,
      "loss": 0.0515,
      "step": 35500
    },
    {
      "epoch": 1.5822222222222222,
      "grad_norm": 0.09022115170955658,
      "learning_rate": 1.2089111111111111e-05,
      "loss": 0.0193,
      "step": 35600
    },
    {
      "epoch": 1.5866666666666667,
      "grad_norm": 1.8423089981079102,
      "learning_rate": 1.206688888888889e-05,
      "loss": 0.0435,
      "step": 35700
    },
    {
      "epoch": 1.5911111111111111,
      "grad_norm": 0.020546812564134598,
      "learning_rate": 1.2044666666666667e-05,
      "loss": 0.0248,
      "step": 35800
    },
    {
      "epoch": 1.5955555555555554,
      "grad_norm": 0.03409857302904129,
      "learning_rate": 1.2022444444444446e-05,
      "loss": 0.0221,
      "step": 35900
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.014954160898923874,
      "learning_rate": 1.2000222222222224e-05,
      "loss": 0.0245,
      "step": 36000
    },
    {
      "epoch": 1.6044444444444443,
      "grad_norm": 0.4946244955062866,
      "learning_rate": 1.1978000000000001e-05,
      "loss": 0.0172,
      "step": 36100
    },
    {
      "epoch": 1.608888888888889,
      "grad_norm": 0.00895380787551403,
      "learning_rate": 1.1955777777777779e-05,
      "loss": 0.0211,
      "step": 36200
    },
    {
      "epoch": 1.6133333333333333,
      "grad_norm": 0.10255364328622818,
      "learning_rate": 1.1933555555555556e-05,
      "loss": 0.0143,
      "step": 36300
    },
    {
      "epoch": 1.6177777777777778,
      "grad_norm": 6.1350626945495605,
      "learning_rate": 1.1911333333333334e-05,
      "loss": 0.0367,
      "step": 36400
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.02449611946940422,
      "learning_rate": 1.1889111111111112e-05,
      "loss": 0.0261,
      "step": 36500
    },
    {
      "epoch": 1.6266666666666667,
      "grad_norm": 0.03168811276555061,
      "learning_rate": 1.186688888888889e-05,
      "loss": 0.0339,
      "step": 36600
    },
    {
      "epoch": 1.6311111111111112,
      "grad_norm": 0.10254495590925217,
      "learning_rate": 1.1844666666666667e-05,
      "loss": 0.0257,
      "step": 36700
    },
    {
      "epoch": 1.6355555555555554,
      "grad_norm": 0.018310539424419403,
      "learning_rate": 1.1822444444444444e-05,
      "loss": 0.0183,
      "step": 36800
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.04140668362379074,
      "learning_rate": 1.1800222222222224e-05,
      "loss": 0.0306,
      "step": 36900
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 5.002046585083008,
      "learning_rate": 1.1778e-05,
      "loss": 0.0354,
      "step": 37000
    },
    {
      "epoch": 1.6488888888888888,
      "grad_norm": 1.8672938346862793,
      "learning_rate": 1.1755777777777779e-05,
      "loss": 0.0446,
      "step": 37100
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 12.282214164733887,
      "learning_rate": 1.1733555555555557e-05,
      "loss": 0.0461,
      "step": 37200
    },
    {
      "epoch": 1.6577777777777778,
      "grad_norm": 0.030551886186003685,
      "learning_rate": 1.1711333333333334e-05,
      "loss": 0.0259,
      "step": 37300
    },
    {
      "epoch": 1.6622222222222223,
      "grad_norm": 0.2500549256801605,
      "learning_rate": 1.1689111111111112e-05,
      "loss": 0.0249,
      "step": 37400
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.01777266152203083,
      "learning_rate": 1.166688888888889e-05,
      "loss": 0.0231,
      "step": 37500
    },
    {
      "epoch": 1.6711111111111112,
      "grad_norm": 0.017271483317017555,
      "learning_rate": 1.1644666666666669e-05,
      "loss": 0.0226,
      "step": 37600
    },
    {
      "epoch": 1.6755555555555555,
      "grad_norm": 4.65861701965332,
      "learning_rate": 1.1622444444444445e-05,
      "loss": 0.0373,
      "step": 37700
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.03591933101415634,
      "learning_rate": 1.1600222222222224e-05,
      "loss": 0.0314,
      "step": 37800
    },
    {
      "epoch": 1.6844444444444444,
      "grad_norm": 0.02525927685201168,
      "learning_rate": 1.1578e-05,
      "loss": 0.0214,
      "step": 37900
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 2.499695062637329,
      "learning_rate": 1.155577777777778e-05,
      "loss": 0.0255,
      "step": 38000
    },
    {
      "epoch": 1.6933333333333334,
      "grad_norm": 0.01528615690767765,
      "learning_rate": 1.1533555555555555e-05,
      "loss": 0.0231,
      "step": 38100
    },
    {
      "epoch": 1.6977777777777778,
      "grad_norm": 0.15673546493053436,
      "learning_rate": 1.1511333333333334e-05,
      "loss": 0.0362,
      "step": 38200
    },
    {
      "epoch": 1.7022222222222223,
      "grad_norm": 0.029126498848199844,
      "learning_rate": 1.1489111111111114e-05,
      "loss": 0.0191,
      "step": 38300
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 0.017597367987036705,
      "learning_rate": 1.146688888888889e-05,
      "loss": 0.0094,
      "step": 38400
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.01082327589392662,
      "learning_rate": 1.1444666666666669e-05,
      "loss": 0.0124,
      "step": 38500
    },
    {
      "epoch": 1.7155555555555555,
      "grad_norm": 0.021486742421984673,
      "learning_rate": 1.1422444444444445e-05,
      "loss": 0.0266,
      "step": 38600
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.03242895007133484,
      "learning_rate": 1.1400222222222224e-05,
      "loss": 0.0193,
      "step": 38700
    },
    {
      "epoch": 1.7244444444444444,
      "grad_norm": 1.5923587083816528,
      "learning_rate": 1.1378e-05,
      "loss": 0.0265,
      "step": 38800
    },
    {
      "epoch": 1.728888888888889,
      "grad_norm": 0.04537127912044525,
      "learning_rate": 1.135577777777778e-05,
      "loss": 0.0199,
      "step": 38900
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.029258549213409424,
      "learning_rate": 1.1333555555555555e-05,
      "loss": 0.0262,
      "step": 39000
    },
    {
      "epoch": 1.7377777777777776,
      "grad_norm": 0.007645277306437492,
      "learning_rate": 1.1311333333333335e-05,
      "loss": 0.0114,
      "step": 39100
    },
    {
      "epoch": 1.7422222222222223,
      "grad_norm": 0.3301815390586853,
      "learning_rate": 1.1289111111111112e-05,
      "loss": 0.0302,
      "step": 39200
    },
    {
      "epoch": 1.7466666666666666,
      "grad_norm": 0.02949366718530655,
      "learning_rate": 1.126688888888889e-05,
      "loss": 0.0305,
      "step": 39300
    },
    {
      "epoch": 1.751111111111111,
      "grad_norm": 0.01253412663936615,
      "learning_rate": 1.1244666666666668e-05,
      "loss": 0.0166,
      "step": 39400
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 0.005741734988987446,
      "learning_rate": 1.1222444444444445e-05,
      "loss": 0.0254,
      "step": 39500
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.03418802097439766,
      "learning_rate": 1.1200222222222223e-05,
      "loss": 0.0316,
      "step": 39600
    },
    {
      "epoch": 1.7644444444444445,
      "grad_norm": 0.03312400355935097,
      "learning_rate": 1.1178e-05,
      "loss": 0.0152,
      "step": 39700
    },
    {
      "epoch": 1.7688888888888887,
      "grad_norm": 0.019020671024918556,
      "learning_rate": 1.1155777777777778e-05,
      "loss": 0.0195,
      "step": 39800
    },
    {
      "epoch": 1.7733333333333334,
      "grad_norm": 0.015376451425254345,
      "learning_rate": 1.1133555555555557e-05,
      "loss": 0.0225,
      "step": 39900
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.016390834003686905,
      "learning_rate": 1.1111333333333333e-05,
      "loss": 0.0174,
      "step": 40000
    },
    {
      "epoch": 1.7822222222222224,
      "grad_norm": 0.007933961227536201,
      "learning_rate": 1.1089111111111113e-05,
      "loss": 0.0145,
      "step": 40100
    },
    {
      "epoch": 1.7866666666666666,
      "grad_norm": 0.032509054988622665,
      "learning_rate": 1.1066888888888888e-05,
      "loss": 0.0284,
      "step": 40200
    },
    {
      "epoch": 1.791111111111111,
      "grad_norm": 0.02033715508878231,
      "learning_rate": 1.1044666666666668e-05,
      "loss": 0.0117,
      "step": 40300
    },
    {
      "epoch": 1.7955555555555556,
      "grad_norm": 0.019047319889068604,
      "learning_rate": 1.1022444444444444e-05,
      "loss": 0.0101,
      "step": 40400
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.00887786690145731,
      "learning_rate": 1.1000222222222223e-05,
      "loss": 0.0076,
      "step": 40500
    },
    {
      "epoch": 1.8044444444444445,
      "grad_norm": 0.004494553431868553,
      "learning_rate": 1.0978000000000002e-05,
      "loss": 0.0121,
      "step": 40600
    },
    {
      "epoch": 1.8088888888888888,
      "grad_norm": 0.02628936432301998,
      "learning_rate": 1.0955777777777778e-05,
      "loss": 0.0345,
      "step": 40700
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 0.0686526969075203,
      "learning_rate": 1.0933555555555558e-05,
      "loss": 0.0356,
      "step": 40800
    },
    {
      "epoch": 1.8177777777777777,
      "grad_norm": 0.01840553991496563,
      "learning_rate": 1.0911333333333333e-05,
      "loss": 0.0223,
      "step": 40900
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 0.021339241415262222,
      "learning_rate": 1.0889111111111113e-05,
      "loss": 0.0415,
      "step": 41000
    },
    {
      "epoch": 1.8266666666666667,
      "grad_norm": 0.024793876335024834,
      "learning_rate": 1.0866888888888889e-05,
      "loss": 0.0293,
      "step": 41100
    },
    {
      "epoch": 1.8311111111111111,
      "grad_norm": 0.018227340653538704,
      "learning_rate": 1.0844666666666668e-05,
      "loss": 0.0248,
      "step": 41200
    },
    {
      "epoch": 1.8355555555555556,
      "grad_norm": 0.014089765958487988,
      "learning_rate": 1.0822444444444444e-05,
      "loss": 0.0226,
      "step": 41300
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.00909151416271925,
      "learning_rate": 1.0800222222222223e-05,
      "loss": 0.0379,
      "step": 41400
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 0.05434752628207207,
      "learning_rate": 1.0778000000000003e-05,
      "loss": 0.0272,
      "step": 41500
    },
    {
      "epoch": 1.8488888888888888,
      "grad_norm": 0.028366558253765106,
      "learning_rate": 1.0755777777777778e-05,
      "loss": 0.0194,
      "step": 41600
    },
    {
      "epoch": 1.8533333333333335,
      "grad_norm": 0.014633698388934135,
      "learning_rate": 1.0733555555555558e-05,
      "loss": 0.0252,
      "step": 41700
    },
    {
      "epoch": 1.8577777777777778,
      "grad_norm": 0.0103066461160779,
      "learning_rate": 1.0711333333333334e-05,
      "loss": 0.0192,
      "step": 41800
    },
    {
      "epoch": 1.8622222222222222,
      "grad_norm": 0.1171882227063179,
      "learning_rate": 1.0689111111111113e-05,
      "loss": 0.0258,
      "step": 41900
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 0.08010297268629074,
      "learning_rate": 1.0666888888888889e-05,
      "loss": 0.0383,
      "step": 42000
    },
    {
      "epoch": 1.871111111111111,
      "grad_norm": 0.17090725898742676,
      "learning_rate": 1.0644666666666668e-05,
      "loss": 0.0206,
      "step": 42100
    },
    {
      "epoch": 1.8755555555555556,
      "grad_norm": 0.021409999579191208,
      "learning_rate": 1.0622444444444446e-05,
      "loss": 0.0336,
      "step": 42200
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.008924304507672787,
      "learning_rate": 1.0600222222222223e-05,
      "loss": 0.0198,
      "step": 42300
    },
    {
      "epoch": 1.8844444444444446,
      "grad_norm": 0.05086459964513779,
      "learning_rate": 1.0578000000000001e-05,
      "loss": 0.0386,
      "step": 42400
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.26049405336380005,
      "learning_rate": 1.0555777777777779e-05,
      "loss": 0.0363,
      "step": 42500
    },
    {
      "epoch": 1.8933333333333333,
      "grad_norm": 0.026524752378463745,
      "learning_rate": 1.0533555555555556e-05,
      "loss": 0.0203,
      "step": 42600
    },
    {
      "epoch": 1.8977777777777778,
      "grad_norm": 0.02443396858870983,
      "learning_rate": 1.0511333333333334e-05,
      "loss": 0.0196,
      "step": 42700
    },
    {
      "epoch": 1.9022222222222223,
      "grad_norm": 0.01432876568287611,
      "learning_rate": 1.0489111111111112e-05,
      "loss": 0.0208,
      "step": 42800
    },
    {
      "epoch": 1.9066666666666667,
      "grad_norm": 0.05802680924534798,
      "learning_rate": 1.0466888888888889e-05,
      "loss": 0.0231,
      "step": 42900
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 0.04593544453382492,
      "learning_rate": 1.0444666666666667e-05,
      "loss": 0.0397,
      "step": 43000
    },
    {
      "epoch": 1.9155555555555557,
      "grad_norm": 0.03215278312563896,
      "learning_rate": 1.0422444444444446e-05,
      "loss": 0.0139,
      "step": 43100
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.020618079230189323,
      "learning_rate": 1.0400222222222222e-05,
      "loss": 0.0267,
      "step": 43200
    },
    {
      "epoch": 1.9244444444444444,
      "grad_norm": 0.02997802384197712,
      "learning_rate": 1.0378000000000001e-05,
      "loss": 0.0281,
      "step": 43300
    },
    {
      "epoch": 1.9288888888888889,
      "grad_norm": 0.020310332998633385,
      "learning_rate": 1.0355777777777777e-05,
      "loss": 0.0204,
      "step": 43400
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 0.009849367663264275,
      "learning_rate": 1.0333555555555556e-05,
      "loss": 0.0215,
      "step": 43500
    },
    {
      "epoch": 1.9377777777777778,
      "grad_norm": 0.008178209885954857,
      "learning_rate": 1.0311333333333332e-05,
      "loss": 0.0133,
      "step": 43600
    },
    {
      "epoch": 1.942222222222222,
      "grad_norm": 0.018197227269411087,
      "learning_rate": 1.0289111111111112e-05,
      "loss": 0.0113,
      "step": 43700
    },
    {
      "epoch": 1.9466666666666668,
      "grad_norm": 7.748833179473877,
      "learning_rate": 1.0266888888888891e-05,
      "loss": 0.035,
      "step": 43800
    },
    {
      "epoch": 1.951111111111111,
      "grad_norm": 0.02960984781384468,
      "learning_rate": 1.0244666666666667e-05,
      "loss": 0.0223,
      "step": 43900
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 0.020772678777575493,
      "learning_rate": 1.0222444444444446e-05,
      "loss": 0.0307,
      "step": 44000
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.023599328473210335,
      "learning_rate": 1.0200222222222222e-05,
      "loss": 0.022,
      "step": 44100
    },
    {
      "epoch": 1.9644444444444444,
      "grad_norm": 0.02157626301050186,
      "learning_rate": 1.0178000000000001e-05,
      "loss": 0.0257,
      "step": 44200
    },
    {
      "epoch": 1.968888888888889,
      "grad_norm": 0.016118301078677177,
      "learning_rate": 1.0155777777777777e-05,
      "loss": 0.0242,
      "step": 44300
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 0.01913880556821823,
      "learning_rate": 1.0133555555555557e-05,
      "loss": 0.0253,
      "step": 44400
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 0.07259182631969452,
      "learning_rate": 1.0111333333333336e-05,
      "loss": 0.0325,
      "step": 44500
    },
    {
      "epoch": 1.982222222222222,
      "grad_norm": 6.073305130004883,
      "learning_rate": 1.0089111111111112e-05,
      "loss": 0.0298,
      "step": 44600
    },
    {
      "epoch": 1.9866666666666668,
      "grad_norm": 0.011225350201129913,
      "learning_rate": 1.0066888888888891e-05,
      "loss": 0.0215,
      "step": 44700
    },
    {
      "epoch": 1.991111111111111,
      "grad_norm": 0.06924594193696976,
      "learning_rate": 1.0044666666666667e-05,
      "loss": 0.0146,
      "step": 44800
    },
    {
      "epoch": 1.9955555555555555,
      "grad_norm": 0.041888728737831116,
      "learning_rate": 1.0022444444444446e-05,
      "loss": 0.0472,
      "step": 44900
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.029866771772503853,
      "learning_rate": 1.0000222222222222e-05,
      "loss": 0.0235,
      "step": 45000
    },
    {
      "epoch": 2.0044444444444443,
      "grad_norm": 20.532079696655273,
      "learning_rate": 9.978000000000002e-06,
      "loss": 0.0195,
      "step": 45100
    },
    {
      "epoch": 2.008888888888889,
      "grad_norm": 0.023276597261428833,
      "learning_rate": 9.95577777777778e-06,
      "loss": 0.0116,
      "step": 45200
    },
    {
      "epoch": 2.013333333333333,
      "grad_norm": 0.07389149814844131,
      "learning_rate": 9.933555555555557e-06,
      "loss": 0.0184,
      "step": 45300
    },
    {
      "epoch": 2.017777777777778,
      "grad_norm": 0.05701902136206627,
      "learning_rate": 9.911333333333335e-06,
      "loss": 0.0252,
      "step": 45400
    },
    {
      "epoch": 2.022222222222222,
      "grad_norm": 0.09376051276922226,
      "learning_rate": 9.889111111111112e-06,
      "loss": 0.0238,
      "step": 45500
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 0.031281109899282455,
      "learning_rate": 9.86688888888889e-06,
      "loss": 0.0194,
      "step": 45600
    },
    {
      "epoch": 2.031111111111111,
      "grad_norm": 0.022569742053747177,
      "learning_rate": 9.844666666666667e-06,
      "loss": 0.012,
      "step": 45700
    },
    {
      "epoch": 2.0355555555555553,
      "grad_norm": 0.016424456611275673,
      "learning_rate": 9.822444444444445e-06,
      "loss": 0.021,
      "step": 45800
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.014712579548358917,
      "learning_rate": 9.800222222222223e-06,
      "loss": 0.0085,
      "step": 45900
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 0.0308996569365263,
      "learning_rate": 9.778e-06,
      "loss": 0.0137,
      "step": 46000
    },
    {
      "epoch": 2.048888888888889,
      "grad_norm": 0.015423169359564781,
      "learning_rate": 9.755777777777778e-06,
      "loss": 0.0109,
      "step": 46100
    },
    {
      "epoch": 2.0533333333333332,
      "grad_norm": 0.10877573490142822,
      "learning_rate": 9.733555555555555e-06,
      "loss": 0.0179,
      "step": 46200
    },
    {
      "epoch": 2.057777777777778,
      "grad_norm": 0.021075138822197914,
      "learning_rate": 9.711333333333333e-06,
      "loss": 0.0196,
      "step": 46300
    },
    {
      "epoch": 2.062222222222222,
      "grad_norm": 0.22887346148490906,
      "learning_rate": 9.68911111111111e-06,
      "loss": 0.0088,
      "step": 46400
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 0.01703936792910099,
      "learning_rate": 9.66688888888889e-06,
      "loss": 0.0329,
      "step": 46500
    },
    {
      "epoch": 2.071111111111111,
      "grad_norm": 0.011411705985665321,
      "learning_rate": 9.644666666666668e-06,
      "loss": 0.0127,
      "step": 46600
    },
    {
      "epoch": 2.0755555555555554,
      "grad_norm": 0.02228422835469246,
      "learning_rate": 9.622444444444445e-06,
      "loss": 0.0134,
      "step": 46700
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.019126560539007187,
      "learning_rate": 9.600222222222223e-06,
      "loss": 0.011,
      "step": 46800
    },
    {
      "epoch": 2.0844444444444443,
      "grad_norm": 0.009724748320877552,
      "learning_rate": 9.578e-06,
      "loss": 0.0146,
      "step": 46900
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 0.06959234923124313,
      "learning_rate": 9.555777777777778e-06,
      "loss": 0.0182,
      "step": 47000
    },
    {
      "epoch": 2.0933333333333333,
      "grad_norm": 0.019661856815218925,
      "learning_rate": 9.533555555555556e-06,
      "loss": 0.005,
      "step": 47100
    },
    {
      "epoch": 2.097777777777778,
      "grad_norm": 0.0064580743201076984,
      "learning_rate": 9.511333333333335e-06,
      "loss": 0.0079,
      "step": 47200
    },
    {
      "epoch": 2.102222222222222,
      "grad_norm": 0.010991825722157955,
      "learning_rate": 9.489111111111113e-06,
      "loss": 0.0061,
      "step": 47300
    },
    {
      "epoch": 2.1066666666666665,
      "grad_norm": 0.005651026498526335,
      "learning_rate": 9.46688888888889e-06,
      "loss": 0.0121,
      "step": 47400
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.03835691139101982,
      "learning_rate": 9.444666666666668e-06,
      "loss": 0.0103,
      "step": 47500
    },
    {
      "epoch": 2.1155555555555554,
      "grad_norm": 0.01490799617022276,
      "learning_rate": 9.422444444444445e-06,
      "loss": 0.008,
      "step": 47600
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.0070220003835856915,
      "learning_rate": 9.400222222222223e-06,
      "loss": 0.0159,
      "step": 47700
    },
    {
      "epoch": 2.1244444444444444,
      "grad_norm": 11.536866188049316,
      "learning_rate": 9.378e-06,
      "loss": 0.0117,
      "step": 47800
    },
    {
      "epoch": 2.128888888888889,
      "grad_norm": 0.009610486216843128,
      "learning_rate": 9.355777777777778e-06,
      "loss": 0.008,
      "step": 47900
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 0.01744682528078556,
      "learning_rate": 9.333555555555558e-06,
      "loss": 0.0081,
      "step": 48000
    },
    {
      "epoch": 2.137777777777778,
      "grad_norm": 0.006743054371327162,
      "learning_rate": 9.311333333333335e-06,
      "loss": 0.0175,
      "step": 48100
    },
    {
      "epoch": 2.1422222222222222,
      "grad_norm": 0.0794791579246521,
      "learning_rate": 9.289111111111113e-06,
      "loss": 0.0252,
      "step": 48200
    },
    {
      "epoch": 2.1466666666666665,
      "grad_norm": 0.030038265511393547,
      "learning_rate": 9.26688888888889e-06,
      "loss": 0.0173,
      "step": 48300
    },
    {
      "epoch": 2.151111111111111,
      "grad_norm": 0.015425505116581917,
      "learning_rate": 9.244666666666668e-06,
      "loss": 0.017,
      "step": 48400
    },
    {
      "epoch": 2.1555555555555554,
      "grad_norm": 0.013013468123972416,
      "learning_rate": 9.222444444444446e-06,
      "loss": 0.0221,
      "step": 48500
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.023410320281982422,
      "learning_rate": 9.200222222222223e-06,
      "loss": 0.0144,
      "step": 48600
    },
    {
      "epoch": 2.1644444444444444,
      "grad_norm": 0.032730165868997574,
      "learning_rate": 9.178000000000001e-06,
      "loss": 0.0084,
      "step": 48700
    },
    {
      "epoch": 2.168888888888889,
      "grad_norm": 0.03105555847287178,
      "learning_rate": 9.155777777777779e-06,
      "loss": 0.0112,
      "step": 48800
    },
    {
      "epoch": 2.1733333333333333,
      "grad_norm": 0.02244781330227852,
      "learning_rate": 9.133555555555556e-06,
      "loss": 0.0159,
      "step": 48900
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 0.009200310334563255,
      "learning_rate": 9.111333333333334e-06,
      "loss": 0.0148,
      "step": 49000
    },
    {
      "epoch": 2.1822222222222223,
      "grad_norm": 0.03619791939854622,
      "learning_rate": 9.089111111111111e-06,
      "loss": 0.02,
      "step": 49100
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 0.020371152088046074,
      "learning_rate": 9.066888888888889e-06,
      "loss": 0.0362,
      "step": 49200
    },
    {
      "epoch": 2.1911111111111112,
      "grad_norm": 0.021619949489831924,
      "learning_rate": 9.044666666666667e-06,
      "loss": 0.0099,
      "step": 49300
    },
    {
      "epoch": 2.1955555555555555,
      "grad_norm": 0.010310902260243893,
      "learning_rate": 9.022444444444444e-06,
      "loss": 0.0128,
      "step": 49400
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.009332481771707535,
      "learning_rate": 9.000222222222222e-06,
      "loss": 0.0146,
      "step": 49500
    },
    {
      "epoch": 2.2044444444444444,
      "grad_norm": 0.005636386573314667,
      "learning_rate": 8.978000000000001e-06,
      "loss": 0.0086,
      "step": 49600
    },
    {
      "epoch": 2.2088888888888887,
      "grad_norm": 0.010973857715725899,
      "learning_rate": 8.955777777777779e-06,
      "loss": 0.0097,
      "step": 49700
    },
    {
      "epoch": 2.2133333333333334,
      "grad_norm": 0.015053996816277504,
      "learning_rate": 8.933555555555556e-06,
      "loss": 0.0258,
      "step": 49800
    },
    {
      "epoch": 2.2177777777777776,
      "grad_norm": 0.008877944201231003,
      "learning_rate": 8.911333333333334e-06,
      "loss": 0.0181,
      "step": 49900
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.010655896738171577,
      "learning_rate": 8.889111111111112e-06,
      "loss": 0.0082,
      "step": 50000
    },
    {
      "epoch": 2.2266666666666666,
      "grad_norm": 0.008763616904616356,
      "learning_rate": 8.86688888888889e-06,
      "loss": 0.0006,
      "step": 50100
    },
    {
      "epoch": 2.2311111111111113,
      "grad_norm": 0.011553546413779259,
      "learning_rate": 8.844666666666667e-06,
      "loss": 0.016,
      "step": 50200
    },
    {
      "epoch": 2.2355555555555555,
      "grad_norm": 0.014006690122187138,
      "learning_rate": 8.822444444444446e-06,
      "loss": 0.0265,
      "step": 50300
    },
    {
      "epoch": 2.24,
      "grad_norm": 123.09770965576172,
      "learning_rate": 8.800222222222224e-06,
      "loss": 0.0241,
      "step": 50400
    },
    {
      "epoch": 2.2444444444444445,
      "grad_norm": 0.012550833635032177,
      "learning_rate": 8.778000000000001e-06,
      "loss": 0.0154,
      "step": 50500
    },
    {
      "epoch": 2.2488888888888887,
      "grad_norm": 0.014727695845067501,
      "learning_rate": 8.755777777777779e-06,
      "loss": 0.0049,
      "step": 50600
    },
    {
      "epoch": 2.2533333333333334,
      "grad_norm": 0.005423196591436863,
      "learning_rate": 8.733555555555557e-06,
      "loss": 0.0031,
      "step": 50700
    },
    {
      "epoch": 2.2577777777777777,
      "grad_norm": 0.27148309350013733,
      "learning_rate": 8.711333333333334e-06,
      "loss": 0.0255,
      "step": 50800
    },
    {
      "epoch": 2.2622222222222224,
      "grad_norm": 0.016477633267641068,
      "learning_rate": 8.689111111111112e-06,
      "loss": 0.0093,
      "step": 50900
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.031139852479100227,
      "learning_rate": 8.66688888888889e-06,
      "loss": 0.0105,
      "step": 51000
    },
    {
      "epoch": 2.2711111111111113,
      "grad_norm": 0.008537497371435165,
      "learning_rate": 8.644666666666669e-06,
      "loss": 0.0064,
      "step": 51100
    },
    {
      "epoch": 2.2755555555555556,
      "grad_norm": 0.008797670714557171,
      "learning_rate": 8.622444444444446e-06,
      "loss": 0.0185,
      "step": 51200
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 0.006916938815265894,
      "learning_rate": 8.600222222222224e-06,
      "loss": 0.0117,
      "step": 51300
    },
    {
      "epoch": 2.2844444444444445,
      "grad_norm": 0.010664207860827446,
      "learning_rate": 8.578000000000002e-06,
      "loss": 0.0126,
      "step": 51400
    },
    {
      "epoch": 2.2888888888888888,
      "grad_norm": 0.010030673816800117,
      "learning_rate": 8.55577777777778e-06,
      "loss": 0.0141,
      "step": 51500
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 0.016369184479117393,
      "learning_rate": 8.533555555555557e-06,
      "loss": 0.0088,
      "step": 51600
    },
    {
      "epoch": 2.2977777777777777,
      "grad_norm": 0.010212864726781845,
      "learning_rate": 8.511333333333334e-06,
      "loss": 0.0085,
      "step": 51700
    },
    {
      "epoch": 2.3022222222222224,
      "grad_norm": 0.02598635107278824,
      "learning_rate": 8.489111111111112e-06,
      "loss": 0.0164,
      "step": 51800
    },
    {
      "epoch": 2.3066666666666666,
      "grad_norm": 0.004812859930098057,
      "learning_rate": 8.46688888888889e-06,
      "loss": 0.0063,
      "step": 51900
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 0.011017361655831337,
      "learning_rate": 8.444666666666667e-06,
      "loss": 0.0193,
      "step": 52000
    },
    {
      "epoch": 2.3155555555555556,
      "grad_norm": 0.0933162197470665,
      "learning_rate": 8.422444444444445e-06,
      "loss": 0.0096,
      "step": 52100
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.017923125997185707,
      "learning_rate": 8.400222222222222e-06,
      "loss": 0.0216,
      "step": 52200
    },
    {
      "epoch": 2.3244444444444445,
      "grad_norm": 0.014297978021204472,
      "learning_rate": 8.378e-06,
      "loss": 0.0219,
      "step": 52300
    },
    {
      "epoch": 2.328888888888889,
      "grad_norm": 0.010282052680850029,
      "learning_rate": 8.355777777777778e-06,
      "loss": 0.0049,
      "step": 52400
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.009734835475683212,
      "learning_rate": 8.333555555555555e-06,
      "loss": 0.0004,
      "step": 52500
    },
    {
      "epoch": 2.3377777777777777,
      "grad_norm": 0.025011537596583366,
      "learning_rate": 8.311333333333333e-06,
      "loss": 0.0199,
      "step": 52600
    },
    {
      "epoch": 2.3422222222222224,
      "grad_norm": 0.02655336819589138,
      "learning_rate": 8.289111111111112e-06,
      "loss": 0.0228,
      "step": 52700
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 0.01286104042083025,
      "learning_rate": 8.26688888888889e-06,
      "loss": 0.0161,
      "step": 52800
    },
    {
      "epoch": 2.351111111111111,
      "grad_norm": 0.012946918606758118,
      "learning_rate": 8.244666666666667e-06,
      "loss": 0.0128,
      "step": 52900
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 0.006833166815340519,
      "learning_rate": 8.222444444444445e-06,
      "loss": 0.0046,
      "step": 53000
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.01692301779985428,
      "learning_rate": 8.200222222222223e-06,
      "loss": 0.0088,
      "step": 53100
    },
    {
      "epoch": 2.3644444444444446,
      "grad_norm": 0.008340106345713139,
      "learning_rate": 8.178e-06,
      "loss": 0.0118,
      "step": 53200
    },
    {
      "epoch": 2.368888888888889,
      "grad_norm": 0.00609036348760128,
      "learning_rate": 8.155777777777778e-06,
      "loss": 0.0082,
      "step": 53300
    },
    {
      "epoch": 2.3733333333333335,
      "grad_norm": 0.006092502269893885,
      "learning_rate": 8.133555555555557e-06,
      "loss": 0.0069,
      "step": 53400
    },
    {
      "epoch": 2.3777777777777778,
      "grad_norm": 0.005671156570315361,
      "learning_rate": 8.111333333333335e-06,
      "loss": 0.0003,
      "step": 53500
    },
    {
      "epoch": 2.3822222222222225,
      "grad_norm": 0.005284800659865141,
      "learning_rate": 8.089111111111112e-06,
      "loss": 0.0177,
      "step": 53600
    },
    {
      "epoch": 2.3866666666666667,
      "grad_norm": 0.012673565186560154,
      "learning_rate": 8.06688888888889e-06,
      "loss": 0.0185,
      "step": 53700
    },
    {
      "epoch": 2.391111111111111,
      "grad_norm": 0.007008051965385675,
      "learning_rate": 8.044666666666668e-06,
      "loss": 0.0063,
      "step": 53800
    },
    {
      "epoch": 2.3955555555555557,
      "grad_norm": 0.0033921271096915007,
      "learning_rate": 8.022444444444445e-06,
      "loss": 0.0027,
      "step": 53900
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.01814904250204563,
      "learning_rate": 8.000222222222223e-06,
      "loss": 0.0133,
      "step": 54000
    },
    {
      "epoch": 2.4044444444444446,
      "grad_norm": 5.543666839599609,
      "learning_rate": 7.978e-06,
      "loss": 0.0302,
      "step": 54100
    },
    {
      "epoch": 2.408888888888889,
      "grad_norm": 6.652066230773926,
      "learning_rate": 7.955777777777778e-06,
      "loss": 0.0243,
      "step": 54200
    },
    {
      "epoch": 2.413333333333333,
      "grad_norm": 0.010969131253659725,
      "learning_rate": 7.933555555555556e-06,
      "loss": 0.0176,
      "step": 54300
    },
    {
      "epoch": 2.417777777777778,
      "grad_norm": 0.012700105085968971,
      "learning_rate": 7.911333333333333e-06,
      "loss": 0.0096,
      "step": 54400
    },
    {
      "epoch": 2.422222222222222,
      "grad_norm": 0.0157205481082201,
      "learning_rate": 7.889111111111113e-06,
      "loss": 0.02,
      "step": 54500
    },
    {
      "epoch": 2.4266666666666667,
      "grad_norm": 0.005976746790111065,
      "learning_rate": 7.86688888888889e-06,
      "loss": 0.0147,
      "step": 54600
    },
    {
      "epoch": 2.431111111111111,
      "grad_norm": 0.008700189180672169,
      "learning_rate": 7.844666666666668e-06,
      "loss": 0.0156,
      "step": 54700
    },
    {
      "epoch": 2.4355555555555557,
      "grad_norm": 0.07866959273815155,
      "learning_rate": 7.822444444444446e-06,
      "loss": 0.0126,
      "step": 54800
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.007938671857118607,
      "learning_rate": 7.800222222222223e-06,
      "loss": 0.0109,
      "step": 54900
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.01021980494260788,
      "learning_rate": 7.778e-06,
      "loss": 0.0137,
      "step": 55000
    },
    {
      "epoch": 2.448888888888889,
      "grad_norm": 0.022627903148531914,
      "learning_rate": 7.755777777777778e-06,
      "loss": 0.0087,
      "step": 55100
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 0.002915649674832821,
      "learning_rate": 7.733555555555556e-06,
      "loss": 0.0041,
      "step": 55200
    },
    {
      "epoch": 2.457777777777778,
      "grad_norm": 0.007160859648138285,
      "learning_rate": 7.711333333333334e-06,
      "loss": 0.0117,
      "step": 55300
    },
    {
      "epoch": 2.462222222222222,
      "grad_norm": 0.00891963578760624,
      "learning_rate": 7.689111111111111e-06,
      "loss": 0.0172,
      "step": 55400
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 0.0076905423775315285,
      "learning_rate": 7.666888888888889e-06,
      "loss": 0.0174,
      "step": 55500
    },
    {
      "epoch": 2.471111111111111,
      "grad_norm": 0.009906003251671791,
      "learning_rate": 7.644666666666666e-06,
      "loss": 0.0123,
      "step": 55600
    },
    {
      "epoch": 2.4755555555555557,
      "grad_norm": 0.004627440124750137,
      "learning_rate": 7.622444444444445e-06,
      "loss": 0.0066,
      "step": 55700
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.017279407009482384,
      "learning_rate": 7.600222222222223e-06,
      "loss": 0.0186,
      "step": 55800
    },
    {
      "epoch": 2.4844444444444447,
      "grad_norm": 0.013360308483242989,
      "learning_rate": 7.578000000000001e-06,
      "loss": 0.0208,
      "step": 55900
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 0.033078521490097046,
      "learning_rate": 7.555777777777779e-06,
      "loss": 0.0124,
      "step": 56000
    },
    {
      "epoch": 2.493333333333333,
      "grad_norm": 0.011481476947665215,
      "learning_rate": 7.533555555555556e-06,
      "loss": 0.0166,
      "step": 56100
    },
    {
      "epoch": 2.497777777777778,
      "grad_norm": 0.04989860951900482,
      "learning_rate": 7.511333333333334e-06,
      "loss": 0.0218,
      "step": 56200
    },
    {
      "epoch": 2.502222222222222,
      "grad_norm": 0.006202944088727236,
      "learning_rate": 7.4891111111111114e-06,
      "loss": 0.0139,
      "step": 56300
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 0.005573649890720844,
      "learning_rate": 7.466888888888889e-06,
      "loss": 0.0048,
      "step": 56400
    },
    {
      "epoch": 2.511111111111111,
      "grad_norm": 0.011436599306762218,
      "learning_rate": 7.4446666666666675e-06,
      "loss": 0.0059,
      "step": 56500
    },
    {
      "epoch": 2.5155555555555553,
      "grad_norm": 0.012748291715979576,
      "learning_rate": 7.422444444444445e-06,
      "loss": 0.0159,
      "step": 56600
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.01532150898128748,
      "learning_rate": 7.400222222222223e-06,
      "loss": 0.0133,
      "step": 56700
    },
    {
      "epoch": 2.5244444444444447,
      "grad_norm": 0.008851067163050175,
      "learning_rate": 7.378e-06,
      "loss": 0.0074,
      "step": 56800
    },
    {
      "epoch": 2.528888888888889,
      "grad_norm": 0.008731141686439514,
      "learning_rate": 7.355777777777778e-06,
      "loss": 0.0099,
      "step": 56900
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.010887227952480316,
      "learning_rate": 7.3335555555555556e-06,
      "loss": 0.01,
      "step": 57000
    },
    {
      "epoch": 2.537777777777778,
      "grad_norm": 0.2833525538444519,
      "learning_rate": 7.311333333333334e-06,
      "loss": 0.0209,
      "step": 57100
    },
    {
      "epoch": 2.542222222222222,
      "grad_norm": 0.012289397418498993,
      "learning_rate": 7.289111111111112e-06,
      "loss": 0.0166,
      "step": 57200
    },
    {
      "epoch": 2.546666666666667,
      "grad_norm": 0.00587312551215291,
      "learning_rate": 7.26688888888889e-06,
      "loss": 0.006,
      "step": 57300
    },
    {
      "epoch": 2.551111111111111,
      "grad_norm": 0.00658233230933547,
      "learning_rate": 7.244666666666668e-06,
      "loss": 0.0036,
      "step": 57400
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.006537387613207102,
      "learning_rate": 7.222444444444445e-06,
      "loss": 0.0083,
      "step": 57500
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.014030762016773224,
      "learning_rate": 7.200222222222223e-06,
      "loss": 0.0283,
      "step": 57600
    },
    {
      "epoch": 2.5644444444444443,
      "grad_norm": 0.026115307584404945,
      "learning_rate": 7.1780000000000006e-06,
      "loss": 0.0128,
      "step": 57700
    },
    {
      "epoch": 2.568888888888889,
      "grad_norm": 0.005813170690089464,
      "learning_rate": 7.155777777777778e-06,
      "loss": 0.007,
      "step": 57800
    },
    {
      "epoch": 2.5733333333333333,
      "grad_norm": 0.0161027479916811,
      "learning_rate": 7.133555555555556e-06,
      "loss": 0.0224,
      "step": 57900
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 0.016258995980024338,
      "learning_rate": 7.111333333333333e-06,
      "loss": 0.0111,
      "step": 58000
    },
    {
      "epoch": 2.582222222222222,
      "grad_norm": 0.011670693755149841,
      "learning_rate": 7.089111111111112e-06,
      "loss": 0.0087,
      "step": 58100
    },
    {
      "epoch": 2.586666666666667,
      "grad_norm": 0.019907653331756592,
      "learning_rate": 7.0668888888888895e-06,
      "loss": 0.0117,
      "step": 58200
    },
    {
      "epoch": 2.591111111111111,
      "grad_norm": 0.0164671428501606,
      "learning_rate": 7.044666666666667e-06,
      "loss": 0.0272,
      "step": 58300
    },
    {
      "epoch": 2.5955555555555554,
      "grad_norm": 0.00677658012136817,
      "learning_rate": 7.022444444444445e-06,
      "loss": 0.0077,
      "step": 58400
    },
    {
      "epoch": 2.6,
      "grad_norm": 0.037212271243333817,
      "learning_rate": 7.000222222222222e-06,
      "loss": 0.0199,
      "step": 58500
    },
    {
      "epoch": 2.6044444444444443,
      "grad_norm": 0.012288598343729973,
      "learning_rate": 6.978e-06,
      "loss": 0.0092,
      "step": 58600
    },
    {
      "epoch": 2.608888888888889,
      "grad_norm": 0.006626170128583908,
      "learning_rate": 6.9557777777777776e-06,
      "loss": 0.0123,
      "step": 58700
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 0.013764744624495506,
      "learning_rate": 6.933555555555556e-06,
      "loss": 0.0141,
      "step": 58800
    },
    {
      "epoch": 2.6177777777777775,
      "grad_norm": 0.024718664586544037,
      "learning_rate": 6.9113333333333345e-06,
      "loss": 0.0046,
      "step": 58900
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 0.010025657713413239,
      "learning_rate": 6.889111111111112e-06,
      "loss": 0.0199,
      "step": 59000
    },
    {
      "epoch": 2.626666666666667,
      "grad_norm": 0.004296141676604748,
      "learning_rate": 6.86688888888889e-06,
      "loss": 0.0188,
      "step": 59100
    },
    {
      "epoch": 2.631111111111111,
      "grad_norm": 0.016384441405534744,
      "learning_rate": 6.844666666666667e-06,
      "loss": 0.0233,
      "step": 59200
    },
    {
      "epoch": 2.6355555555555554,
      "grad_norm": 0.006286513060331345,
      "learning_rate": 6.822444444444445e-06,
      "loss": 0.0059,
      "step": 59300
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.008424772880971432,
      "learning_rate": 6.8002222222222225e-06,
      "loss": 0.0093,
      "step": 59400
    },
    {
      "epoch": 2.6444444444444444,
      "grad_norm": 0.007219286169856787,
      "learning_rate": 6.778e-06,
      "loss": 0.0087,
      "step": 59500
    },
    {
      "epoch": 2.648888888888889,
      "grad_norm": 0.01014529075473547,
      "learning_rate": 6.755777777777779e-06,
      "loss": 0.0154,
      "step": 59600
    },
    {
      "epoch": 2.6533333333333333,
      "grad_norm": 0.0056380704045295715,
      "learning_rate": 6.733555555555556e-06,
      "loss": 0.0108,
      "step": 59700
    },
    {
      "epoch": 2.6577777777777776,
      "grad_norm": 0.006083510350435972,
      "learning_rate": 6.711333333333334e-06,
      "loss": 0.0043,
      "step": 59800
    },
    {
      "epoch": 2.6622222222222223,
      "grad_norm": 0.11660513281822205,
      "learning_rate": 6.6891111111111115e-06,
      "loss": 0.0132,
      "step": 59900
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.009573640301823616,
      "learning_rate": 6.666888888888889e-06,
      "loss": 0.0208,
      "step": 60000
    },
    {
      "epoch": 2.671111111111111,
      "grad_norm": 0.01808933913707733,
      "learning_rate": 6.644666666666667e-06,
      "loss": 0.0144,
      "step": 60100
    },
    {
      "epoch": 2.6755555555555555,
      "grad_norm": 0.011520824395120144,
      "learning_rate": 6.622444444444444e-06,
      "loss": 0.0081,
      "step": 60200
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.00454321876168251,
      "learning_rate": 6.600222222222222e-06,
      "loss": 0.0038,
      "step": 60300
    },
    {
      "epoch": 2.6844444444444444,
      "grad_norm": 0.004184955731034279,
      "learning_rate": 6.578000000000001e-06,
      "loss": 0.0076,
      "step": 60400
    },
    {
      "epoch": 2.688888888888889,
      "grad_norm": 0.004420077428221703,
      "learning_rate": 6.555777777777779e-06,
      "loss": 0.0127,
      "step": 60500
    },
    {
      "epoch": 2.6933333333333334,
      "grad_norm": 0.005254014395177364,
      "learning_rate": 6.5335555555555565e-06,
      "loss": 0.0167,
      "step": 60600
    },
    {
      "epoch": 2.6977777777777776,
      "grad_norm": 0.008908220566809177,
      "learning_rate": 6.511333333333334e-06,
      "loss": 0.0053,
      "step": 60700
    },
    {
      "epoch": 2.7022222222222223,
      "grad_norm": 0.016256263479590416,
      "learning_rate": 6.489111111111112e-06,
      "loss": 0.0064,
      "step": 60800
    },
    {
      "epoch": 2.7066666666666666,
      "grad_norm": 4.987905979156494,
      "learning_rate": 6.466888888888889e-06,
      "loss": 0.0137,
      "step": 60900
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 0.00586308166384697,
      "learning_rate": 6.444666666666667e-06,
      "loss": 0.0097,
      "step": 61000
    },
    {
      "epoch": 2.7155555555555555,
      "grad_norm": 0.0063577983528375626,
      "learning_rate": 6.4224444444444445e-06,
      "loss": 0.0109,
      "step": 61100
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.013244284316897392,
      "learning_rate": 6.400222222222223e-06,
      "loss": 0.0116,
      "step": 61200
    },
    {
      "epoch": 2.7244444444444444,
      "grad_norm": 0.00379652576521039,
      "learning_rate": 6.378000000000001e-06,
      "loss": 0.0058,
      "step": 61300
    },
    {
      "epoch": 2.728888888888889,
      "grad_norm": 0.010458216071128845,
      "learning_rate": 6.355777777777778e-06,
      "loss": 0.0181,
      "step": 61400
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 0.005135597661137581,
      "learning_rate": 6.333555555555556e-06,
      "loss": 0.0004,
      "step": 61500
    },
    {
      "epoch": 2.7377777777777776,
      "grad_norm": 0.009778694249689579,
      "learning_rate": 6.3113333333333334e-06,
      "loss": 0.0203,
      "step": 61600
    },
    {
      "epoch": 2.7422222222222223,
      "grad_norm": 0.011632485315203667,
      "learning_rate": 6.289111111111111e-06,
      "loss": 0.0154,
      "step": 61700
    },
    {
      "epoch": 2.7466666666666666,
      "grad_norm": 0.0053710294887423515,
      "learning_rate": 6.266888888888889e-06,
      "loss": 0.0085,
      "step": 61800
    },
    {
      "epoch": 2.7511111111111113,
      "grad_norm": 0.016084793955087662,
      "learning_rate": 6.244666666666666e-06,
      "loss": 0.0098,
      "step": 61900
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 0.00773009704425931,
      "learning_rate": 6.222444444444446e-06,
      "loss": 0.0186,
      "step": 62000
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.015772517770528793,
      "learning_rate": 6.200222222222223e-06,
      "loss": 0.0102,
      "step": 62100
    },
    {
      "epoch": 2.7644444444444445,
      "grad_norm": 0.0029959585517644882,
      "learning_rate": 6.178000000000001e-06,
      "loss": 0.0069,
      "step": 62200
    },
    {
      "epoch": 2.7688888888888887,
      "grad_norm": 0.009035189636051655,
      "learning_rate": 6.1557777777777784e-06,
      "loss": 0.012,
      "step": 62300
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 0.012829063460230827,
      "learning_rate": 6.133555555555556e-06,
      "loss": 0.0168,
      "step": 62400
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.008505674079060555,
      "learning_rate": 6.111333333333334e-06,
      "loss": 0.0267,
      "step": 62500
    },
    {
      "epoch": 2.7822222222222224,
      "grad_norm": 0.010427444241940975,
      "learning_rate": 6.089111111111111e-06,
      "loss": 0.0183,
      "step": 62600
    },
    {
      "epoch": 2.7866666666666666,
      "grad_norm": 0.006591698154807091,
      "learning_rate": 6.06688888888889e-06,
      "loss": 0.0057,
      "step": 62700
    },
    {
      "epoch": 2.7911111111111113,
      "grad_norm": 0.005993884056806564,
      "learning_rate": 6.044666666666667e-06,
      "loss": 0.003,
      "step": 62800
    },
    {
      "epoch": 2.7955555555555556,
      "grad_norm": 0.03321168199181557,
      "learning_rate": 6.022444444444445e-06,
      "loss": 0.0276,
      "step": 62900
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.058045003563165665,
      "learning_rate": 6.000222222222223e-06,
      "loss": 0.017,
      "step": 63000
    },
    {
      "epoch": 2.8044444444444445,
      "grad_norm": 0.00500097731128335,
      "learning_rate": 5.978e-06,
      "loss": 0.0131,
      "step": 63100
    },
    {
      "epoch": 2.8088888888888888,
      "grad_norm": 6.233802795410156,
      "learning_rate": 5.955777777777778e-06,
      "loss": 0.0217,
      "step": 63200
    },
    {
      "epoch": 2.8133333333333335,
      "grad_norm": 0.009549891576170921,
      "learning_rate": 5.933555555555555e-06,
      "loss": 0.0012,
      "step": 63300
    },
    {
      "epoch": 2.8177777777777777,
      "grad_norm": 0.012334716506302357,
      "learning_rate": 5.911333333333333e-06,
      "loss": 0.009,
      "step": 63400
    },
    {
      "epoch": 2.822222222222222,
      "grad_norm": 0.011423046700656414,
      "learning_rate": 5.889111111111112e-06,
      "loss": 0.0097,
      "step": 63500
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 0.04938090220093727,
      "learning_rate": 5.86688888888889e-06,
      "loss": 0.0078,
      "step": 63600
    },
    {
      "epoch": 2.8311111111111114,
      "grad_norm": 0.010827896185219288,
      "learning_rate": 5.8446666666666676e-06,
      "loss": 0.0105,
      "step": 63700
    },
    {
      "epoch": 2.8355555555555556,
      "grad_norm": 0.016069572418928146,
      "learning_rate": 5.822444444444445e-06,
      "loss": 0.004,
      "step": 63800
    },
    {
      "epoch": 2.84,
      "grad_norm": 0.028348466381430626,
      "learning_rate": 5.800222222222223e-06,
      "loss": 0.0196,
      "step": 63900
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 0.008137369528412819,
      "learning_rate": 5.778e-06,
      "loss": 0.0253,
      "step": 64000
    },
    {
      "epoch": 2.848888888888889,
      "grad_norm": 0.014171035960316658,
      "learning_rate": 5.755777777777778e-06,
      "loss": 0.0064,
      "step": 64100
    },
    {
      "epoch": 2.8533333333333335,
      "grad_norm": 0.011692403815686703,
      "learning_rate": 5.733555555555556e-06,
      "loss": 0.0096,
      "step": 64200
    },
    {
      "epoch": 2.8577777777777778,
      "grad_norm": 0.02651355229318142,
      "learning_rate": 5.711333333333334e-06,
      "loss": 0.0255,
      "step": 64300
    },
    {
      "epoch": 2.862222222222222,
      "grad_norm": 0.014960981905460358,
      "learning_rate": 5.689111111111112e-06,
      "loss": 0.0193,
      "step": 64400
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 0.027529368177056313,
      "learning_rate": 5.666888888888889e-06,
      "loss": 0.0222,
      "step": 64500
    },
    {
      "epoch": 2.871111111111111,
      "grad_norm": 0.011654230765998363,
      "learning_rate": 5.644666666666667e-06,
      "loss": 0.0102,
      "step": 64600
    },
    {
      "epoch": 2.8755555555555556,
      "grad_norm": 4.548903465270996,
      "learning_rate": 5.6224444444444446e-06,
      "loss": 0.0108,
      "step": 64700
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.03093881532549858,
      "learning_rate": 5.600222222222222e-06,
      "loss": 0.0049,
      "step": 64800
    },
    {
      "epoch": 2.8844444444444446,
      "grad_norm": 0.0065957303158938885,
      "learning_rate": 5.578e-06,
      "loss": 0.0004,
      "step": 64900
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.004011430311948061,
      "learning_rate": 5.555777777777777e-06,
      "loss": 0.0254,
      "step": 65000
    },
    {
      "epoch": 2.8933333333333335,
      "grad_norm": 0.005633799359202385,
      "learning_rate": 5.533555555555557e-06,
      "loss": 0.0044,
      "step": 65100
    },
    {
      "epoch": 2.897777777777778,
      "grad_norm": 0.0056043220683932304,
      "learning_rate": 5.511333333333334e-06,
      "loss": 0.0151,
      "step": 65200
    },
    {
      "epoch": 2.902222222222222,
      "grad_norm": 0.011884791776537895,
      "learning_rate": 5.489111111111112e-06,
      "loss": 0.0108,
      "step": 65300
    },
    {
      "epoch": 2.9066666666666667,
      "grad_norm": 0.0096850311383605,
      "learning_rate": 5.4668888888888896e-06,
      "loss": 0.0179,
      "step": 65400
    },
    {
      "epoch": 2.911111111111111,
      "grad_norm": 0.010264099575579166,
      "learning_rate": 5.444666666666667e-06,
      "loss": 0.0311,
      "step": 65500
    },
    {
      "epoch": 2.9155555555555557,
      "grad_norm": 0.00549297034740448,
      "learning_rate": 5.422444444444445e-06,
      "loss": 0.0097,
      "step": 65600
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.007742481306195259,
      "learning_rate": 5.400222222222222e-06,
      "loss": 0.0139,
      "step": 65700
    },
    {
      "epoch": 2.924444444444444,
      "grad_norm": 0.024677705019712448,
      "learning_rate": 5.378e-06,
      "loss": 0.0246,
      "step": 65800
    },
    {
      "epoch": 2.928888888888889,
      "grad_norm": 0.027099808678030968,
      "learning_rate": 5.3557777777777785e-06,
      "loss": 0.0208,
      "step": 65900
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 0.009479946456849575,
      "learning_rate": 5.333555555555556e-06,
      "loss": 0.0069,
      "step": 66000
    },
    {
      "epoch": 2.937777777777778,
      "grad_norm": 0.016817275434732437,
      "learning_rate": 5.311333333333334e-06,
      "loss": 0.0088,
      "step": 66100
    },
    {
      "epoch": 2.942222222222222,
      "grad_norm": 0.008676017634570599,
      "learning_rate": 5.289111111111111e-06,
      "loss": 0.017,
      "step": 66200
    },
    {
      "epoch": 2.9466666666666668,
      "grad_norm": 0.006666612811386585,
      "learning_rate": 5.266888888888889e-06,
      "loss": 0.0116,
      "step": 66300
    },
    {
      "epoch": 2.951111111111111,
      "grad_norm": 0.03567556291818619,
      "learning_rate": 5.2446666666666665e-06,
      "loss": 0.0174,
      "step": 66400
    },
    {
      "epoch": 2.9555555555555557,
      "grad_norm": 0.008006120100617409,
      "learning_rate": 5.222444444444444e-06,
      "loss": 0.0084,
      "step": 66500
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.012151412665843964,
      "learning_rate": 5.2002222222222235e-06,
      "loss": 0.0004,
      "step": 66600
    },
    {
      "epoch": 2.964444444444444,
      "grad_norm": 0.029078492894768715,
      "learning_rate": 5.178000000000001e-06,
      "loss": 0.022,
      "step": 66700
    },
    {
      "epoch": 2.968888888888889,
      "grad_norm": 0.007081541232764721,
      "learning_rate": 5.155777777777779e-06,
      "loss": 0.0079,
      "step": 66800
    },
    {
      "epoch": 2.9733333333333336,
      "grad_norm": 0.010919651947915554,
      "learning_rate": 5.133555555555556e-06,
      "loss": 0.0161,
      "step": 66900
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 0.019095251336693764,
      "learning_rate": 5.111333333333334e-06,
      "loss": 0.0067,
      "step": 67000
    },
    {
      "epoch": 2.982222222222222,
      "grad_norm": 0.011317308060824871,
      "learning_rate": 5.0891111111111115e-06,
      "loss": 0.008,
      "step": 67100
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 0.05324229225516319,
      "learning_rate": 5.066888888888889e-06,
      "loss": 0.0082,
      "step": 67200
    },
    {
      "epoch": 2.991111111111111,
      "grad_norm": 0.008243183605372906,
      "learning_rate": 5.044666666666667e-06,
      "loss": 0.0151,
      "step": 67300
    },
    {
      "epoch": 2.9955555555555557,
      "grad_norm": 0.0070766545832157135,
      "learning_rate": 5.022444444444445e-06,
      "loss": 0.0093,
      "step": 67400
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.010804212652146816,
      "learning_rate": 5.000222222222223e-06,
      "loss": 0.01,
      "step": 67500
    },
    {
      "epoch": 3.0044444444444443,
      "grad_norm": 0.005016440991312265,
      "learning_rate": 4.9780000000000005e-06,
      "loss": 0.0004,
      "step": 67600
    },
    {
      "epoch": 3.008888888888889,
      "grad_norm": 0.004651058930903673,
      "learning_rate": 4.955777777777778e-06,
      "loss": 0.0092,
      "step": 67700
    },
    {
      "epoch": 3.013333333333333,
      "grad_norm": 0.007044182159006596,
      "learning_rate": 4.933555555555556e-06,
      "loss": 0.0033,
      "step": 67800
    },
    {
      "epoch": 3.017777777777778,
      "grad_norm": 0.006861164700239897,
      "learning_rate": 4.911333333333333e-06,
      "loss": 0.0003,
      "step": 67900
    },
    {
      "epoch": 3.022222222222222,
      "grad_norm": 2.6845309734344482,
      "learning_rate": 4.889111111111112e-06,
      "loss": 0.0113,
      "step": 68000
    },
    {
      "epoch": 3.026666666666667,
      "grad_norm": 0.01601027138531208,
      "learning_rate": 4.866888888888889e-06,
      "loss": 0.0123,
      "step": 68100
    },
    {
      "epoch": 3.031111111111111,
      "grad_norm": 0.01427297294139862,
      "learning_rate": 4.844666666666667e-06,
      "loss": 0.005,
      "step": 68200
    },
    {
      "epoch": 3.0355555555555553,
      "grad_norm": 0.005796924699097872,
      "learning_rate": 4.822444444444445e-06,
      "loss": 0.0131,
      "step": 68300
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.00678290706127882,
      "learning_rate": 4.800222222222223e-06,
      "loss": 0.0059,
      "step": 68400
    },
    {
      "epoch": 3.0444444444444443,
      "grad_norm": 0.0063263894990086555,
      "learning_rate": 4.778000000000001e-06,
      "loss": 0.0003,
      "step": 68500
    },
    {
      "epoch": 3.048888888888889,
      "grad_norm": 0.008038574829697609,
      "learning_rate": 4.755777777777778e-06,
      "loss": 0.0079,
      "step": 68600
    },
    {
      "epoch": 3.0533333333333332,
      "grad_norm": 0.004635456949472427,
      "learning_rate": 4.733555555555556e-06,
      "loss": 0.0093,
      "step": 68700
    },
    {
      "epoch": 3.057777777777778,
      "grad_norm": 0.00704146595671773,
      "learning_rate": 4.7113333333333335e-06,
      "loss": 0.0049,
      "step": 68800
    },
    {
      "epoch": 3.062222222222222,
      "grad_norm": 0.007737556006759405,
      "learning_rate": 4.689111111111111e-06,
      "loss": 0.0002,
      "step": 68900
    },
    {
      "epoch": 3.066666666666667,
      "grad_norm": 0.005160603206604719,
      "learning_rate": 4.666888888888889e-06,
      "loss": 0.0174,
      "step": 69000
    },
    {
      "epoch": 3.071111111111111,
      "grad_norm": 0.005335725843906403,
      "learning_rate": 4.644666666666667e-06,
      "loss": 0.0069,
      "step": 69100
    },
    {
      "epoch": 3.0755555555555554,
      "grad_norm": 0.004169610794633627,
      "learning_rate": 4.622444444444445e-06,
      "loss": 0.0002,
      "step": 69200
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.00658772187307477,
      "learning_rate": 4.6002222222222224e-06,
      "loss": 0.0047,
      "step": 69300
    },
    {
      "epoch": 3.0844444444444443,
      "grad_norm": 0.004493979271501303,
      "learning_rate": 4.578e-06,
      "loss": 0.019,
      "step": 69400
    },
    {
      "epoch": 3.088888888888889,
      "grad_norm": 0.0049393679946660995,
      "learning_rate": 4.5557777777777785e-06,
      "loss": 0.0038,
      "step": 69500
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 0.008688095957040787,
      "learning_rate": 4.533555555555556e-06,
      "loss": 0.0026,
      "step": 69600
    },
    {
      "epoch": 3.097777777777778,
      "grad_norm": 0.005579312797635794,
      "learning_rate": 4.511333333333334e-06,
      "loss": 0.003,
      "step": 69700
    },
    {
      "epoch": 3.102222222222222,
      "grad_norm": 0.008930541574954987,
      "learning_rate": 4.489111111111111e-06,
      "loss": 0.012,
      "step": 69800
    },
    {
      "epoch": 3.1066666666666665,
      "grad_norm": 0.9905691742897034,
      "learning_rate": 4.46688888888889e-06,
      "loss": 0.0003,
      "step": 69900
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 0.019825002178549767,
      "learning_rate": 4.444666666666667e-06,
      "loss": 0.0132,
      "step": 70000
    },
    {
      "epoch": 3.1155555555555554,
      "grad_norm": 0.0023422010708600283,
      "learning_rate": 4.422444444444445e-06,
      "loss": 0.0049,
      "step": 70100
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.0041359527967870235,
      "learning_rate": 4.400222222222223e-06,
      "loss": 0.0002,
      "step": 70200
    },
    {
      "epoch": 3.1244444444444444,
      "grad_norm": 0.0047105406410992146,
      "learning_rate": 4.378e-06,
      "loss": 0.0092,
      "step": 70300
    },
    {
      "epoch": 3.128888888888889,
      "grad_norm": 0.00326539552770555,
      "learning_rate": 4.355777777777778e-06,
      "loss": 0.0159,
      "step": 70400
    },
    {
      "epoch": 3.1333333333333333,
      "grad_norm": 0.053041741251945496,
      "learning_rate": 4.3335555555555555e-06,
      "loss": 0.0051,
      "step": 70500
    },
    {
      "epoch": 3.137777777777778,
      "grad_norm": 0.0021455700043588877,
      "learning_rate": 4.311333333333333e-06,
      "loss": 0.0035,
      "step": 70600
    },
    {
      "epoch": 3.1422222222222222,
      "grad_norm": 0.003934561740607023,
      "learning_rate": 4.2891111111111116e-06,
      "loss": 0.0048,
      "step": 70700
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 0.003967362456023693,
      "learning_rate": 4.266888888888889e-06,
      "loss": 0.0002,
      "step": 70800
    },
    {
      "epoch": 3.151111111111111,
      "grad_norm": 0.003521761391311884,
      "learning_rate": 4.244666666666667e-06,
      "loss": 0.0045,
      "step": 70900
    },
    {
      "epoch": 3.1555555555555554,
      "grad_norm": 0.002769996877759695,
      "learning_rate": 4.222444444444444e-06,
      "loss": 0.0104,
      "step": 71000
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.00691636698320508,
      "learning_rate": 4.200222222222223e-06,
      "loss": 0.0073,
      "step": 71100
    },
    {
      "epoch": 3.1644444444444444,
      "grad_norm": 0.0037927282974123955,
      "learning_rate": 4.1780000000000005e-06,
      "loss": 0.0058,
      "step": 71200
    },
    {
      "epoch": 3.168888888888889,
      "grad_norm": 0.003060796530917287,
      "learning_rate": 4.155777777777778e-06,
      "loss": 0.0006,
      "step": 71300
    },
    {
      "epoch": 3.1733333333333333,
      "grad_norm": 0.053395673632621765,
      "learning_rate": 4.133555555555556e-06,
      "loss": 0.0039,
      "step": 71400
    },
    {
      "epoch": 3.1777777777777776,
      "grad_norm": 0.0028225970454514027,
      "learning_rate": 4.111333333333334e-06,
      "loss": 0.0001,
      "step": 71500
    },
    {
      "epoch": 3.1822222222222223,
      "grad_norm": 0.0013932245783507824,
      "learning_rate": 4.089111111111112e-06,
      "loss": 0.0001,
      "step": 71600
    },
    {
      "epoch": 3.1866666666666665,
      "grad_norm": 0.0029657604172825813,
      "learning_rate": 4.066888888888889e-06,
      "loss": 0.0098,
      "step": 71700
    },
    {
      "epoch": 3.1911111111111112,
      "grad_norm": 0.0013725346652790904,
      "learning_rate": 4.044666666666667e-06,
      "loss": 0.0089,
      "step": 71800
    },
    {
      "epoch": 3.1955555555555555,
      "grad_norm": 0.0038646094035357237,
      "learning_rate": 4.022444444444445e-06,
      "loss": 0.0142,
      "step": 71900
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.04209320247173309,
      "learning_rate": 4.000222222222222e-06,
      "loss": 0.0053,
      "step": 72000
    },
    {
      "epoch": 3.2044444444444444,
      "grad_norm": 0.00380904832854867,
      "learning_rate": 3.978e-06,
      "loss": 0.0176,
      "step": 72100
    },
    {
      "epoch": 3.2088888888888887,
      "grad_norm": 0.0040087150409817696,
      "learning_rate": 3.9557777777777775e-06,
      "loss": 0.0002,
      "step": 72200
    },
    {
      "epoch": 3.2133333333333334,
      "grad_norm": 0.0040077404119074345,
      "learning_rate": 3.933555555555556e-06,
      "loss": 0.0079,
      "step": 72300
    },
    {
      "epoch": 3.2177777777777776,
      "grad_norm": 0.0021754815243184566,
      "learning_rate": 3.9113333333333335e-06,
      "loss": 0.0004,
      "step": 72400
    },
    {
      "epoch": 3.2222222222222223,
      "grad_norm": 0.002889450639486313,
      "learning_rate": 3.889111111111111e-06,
      "loss": 0.0029,
      "step": 72500
    },
    {
      "epoch": 3.2266666666666666,
      "grad_norm": 0.004129678942263126,
      "learning_rate": 3.86688888888889e-06,
      "loss": 0.0088,
      "step": 72600
    },
    {
      "epoch": 3.2311111111111113,
      "grad_norm": 2.3338048458099365,
      "learning_rate": 3.844666666666667e-06,
      "loss": 0.0083,
      "step": 72700
    },
    {
      "epoch": 3.2355555555555555,
      "grad_norm": 0.003920591901987791,
      "learning_rate": 3.822444444444445e-06,
      "loss": 0.0126,
      "step": 72800
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.0030196430161595345,
      "learning_rate": 3.8002222222222225e-06,
      "loss": 0.0041,
      "step": 72900
    },
    {
      "epoch": 3.2444444444444445,
      "grad_norm": 0.0030579501762986183,
      "learning_rate": 3.7780000000000005e-06,
      "loss": 0.0003,
      "step": 73000
    },
    {
      "epoch": 3.2488888888888887,
      "grad_norm": 0.009829430840909481,
      "learning_rate": 3.755777777777778e-06,
      "loss": 0.0055,
      "step": 73100
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 0.0035386737436056137,
      "learning_rate": 3.7335555555555557e-06,
      "loss": 0.0039,
      "step": 73200
    },
    {
      "epoch": 3.2577777777777777,
      "grad_norm": 0.0037764161825180054,
      "learning_rate": 3.7113333333333333e-06,
      "loss": 0.0038,
      "step": 73300
    },
    {
      "epoch": 3.2622222222222224,
      "grad_norm": 0.003624163568019867,
      "learning_rate": 3.689111111111112e-06,
      "loss": 0.0109,
      "step": 73400
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.003711588215082884,
      "learning_rate": 3.6668888888888894e-06,
      "loss": 0.0183,
      "step": 73500
    },
    {
      "epoch": 3.2711111111111113,
      "grad_norm": 0.0024842529091984034,
      "learning_rate": 3.644666666666667e-06,
      "loss": 0.005,
      "step": 73600
    },
    {
      "epoch": 3.2755555555555556,
      "grad_norm": 0.8861402869224548,
      "learning_rate": 3.6224444444444447e-06,
      "loss": 0.0123,
      "step": 73700
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.005019760224968195,
      "learning_rate": 3.6002222222222227e-06,
      "loss": 0.0053,
      "step": 73800
    },
    {
      "epoch": 3.2844444444444445,
      "grad_norm": 0.005349472630769014,
      "learning_rate": 3.5780000000000003e-06,
      "loss": 0.0028,
      "step": 73900
    },
    {
      "epoch": 3.2888888888888888,
      "grad_norm": 0.004001416265964508,
      "learning_rate": 3.555777777777778e-06,
      "loss": 0.0054,
      "step": 74000
    },
    {
      "epoch": 3.2933333333333334,
      "grad_norm": 0.0027487040497362614,
      "learning_rate": 3.5335555555555555e-06,
      "loss": 0.0064,
      "step": 74100
    },
    {
      "epoch": 3.2977777777777777,
      "grad_norm": 0.002847102703526616,
      "learning_rate": 3.511333333333334e-06,
      "loss": 0.0051,
      "step": 74200
    },
    {
      "epoch": 3.3022222222222224,
      "grad_norm": 0.0018439498962834477,
      "learning_rate": 3.4891111111111116e-06,
      "loss": 0.0002,
      "step": 74300
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 0.0033370759338140488,
      "learning_rate": 3.4668888888888892e-06,
      "loss": 0.0052,
      "step": 74400
    },
    {
      "epoch": 3.311111111111111,
      "grad_norm": 0.002945124637335539,
      "learning_rate": 3.444666666666667e-06,
      "loss": 0.0044,
      "step": 74500
    },
    {
      "epoch": 3.3155555555555556,
      "grad_norm": 0.004454643465578556,
      "learning_rate": 3.422444444444445e-06,
      "loss": 0.0098,
      "step": 74600
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.005273269489407539,
      "learning_rate": 3.4002222222222225e-06,
      "loss": 0.0017,
      "step": 74700
    },
    {
      "epoch": 3.3244444444444445,
      "grad_norm": 0.003369648475199938,
      "learning_rate": 3.378e-06,
      "loss": 0.0001,
      "step": 74800
    },
    {
      "epoch": 3.328888888888889,
      "grad_norm": 0.0033572546672075987,
      "learning_rate": 3.3557777777777777e-06,
      "loss": 0.0099,
      "step": 74900
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.004919103346765041,
      "learning_rate": 3.333555555555556e-06,
      "loss": 0.0002,
      "step": 75000
    },
    {
      "epoch": 3.3377777777777777,
      "grad_norm": 0.0008628934156149626,
      "learning_rate": 3.311333333333334e-06,
      "loss": 0.0001,
      "step": 75100
    },
    {
      "epoch": 3.3422222222222224,
      "grad_norm": 0.0043428451754152775,
      "learning_rate": 3.2891111111111114e-06,
      "loss": 0.0045,
      "step": 75200
    },
    {
      "epoch": 3.3466666666666667,
      "grad_norm": 0.004262737464159727,
      "learning_rate": 3.266888888888889e-06,
      "loss": 0.0141,
      "step": 75300
    },
    {
      "epoch": 3.351111111111111,
      "grad_norm": 0.002515859203413129,
      "learning_rate": 3.244666666666667e-06,
      "loss": 0.0151,
      "step": 75400
    },
    {
      "epoch": 3.3555555555555556,
      "grad_norm": 0.004067851696163416,
      "learning_rate": 3.2224444444444447e-06,
      "loss": 0.01,
      "step": 75500
    },
    {
      "epoch": 3.36,
      "grad_norm": 0.001912127248942852,
      "learning_rate": 3.2002222222222223e-06,
      "loss": 0.0089,
      "step": 75600
    },
    {
      "epoch": 3.3644444444444446,
      "grad_norm": 0.004026451613754034,
      "learning_rate": 3.1780000000000003e-06,
      "loss": 0.0041,
      "step": 75700
    },
    {
      "epoch": 3.368888888888889,
      "grad_norm": 0.0035873819142580032,
      "learning_rate": 3.1557777777777784e-06,
      "loss": 0.0047,
      "step": 75800
    },
    {
      "epoch": 3.3733333333333335,
      "grad_norm": 0.0057237958535552025,
      "learning_rate": 3.133555555555556e-06,
      "loss": 0.0077,
      "step": 75900
    },
    {
      "epoch": 3.3777777777777778,
      "grad_norm": 0.0048365602269768715,
      "learning_rate": 3.1113333333333336e-06,
      "loss": 0.0002,
      "step": 76000
    },
    {
      "epoch": 3.3822222222222225,
      "grad_norm": 0.005033082328736782,
      "learning_rate": 3.0891111111111116e-06,
      "loss": 0.0065,
      "step": 76100
    },
    {
      "epoch": 3.3866666666666667,
      "grad_norm": 0.006377544719725847,
      "learning_rate": 3.0668888888888892e-06,
      "loss": 0.0144,
      "step": 76200
    },
    {
      "epoch": 3.391111111111111,
      "grad_norm": 0.004295991733670235,
      "learning_rate": 3.044666666666667e-06,
      "loss": 0.0098,
      "step": 76300
    },
    {
      "epoch": 3.3955555555555557,
      "grad_norm": 0.0032095208298414946,
      "learning_rate": 3.0224444444444445e-06,
      "loss": 0.0013,
      "step": 76400
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.0025091751012951136,
      "learning_rate": 3.0002222222222225e-06,
      "loss": 0.01,
      "step": 76500
    },
    {
      "epoch": 3.4044444444444446,
      "grad_norm": 0.06016986072063446,
      "learning_rate": 2.9780000000000005e-06,
      "loss": 0.02,
      "step": 76600
    },
    {
      "epoch": 3.408888888888889,
      "grad_norm": 0.01066031027585268,
      "learning_rate": 2.955777777777778e-06,
      "loss": 0.0051,
      "step": 76700
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 0.005530976690351963,
      "learning_rate": 2.9335555555555558e-06,
      "loss": 0.0003,
      "step": 76800
    },
    {
      "epoch": 3.417777777777778,
      "grad_norm": 0.004014287609606981,
      "learning_rate": 2.911333333333334e-06,
      "loss": 0.0049,
      "step": 76900
    },
    {
      "epoch": 3.422222222222222,
      "grad_norm": 0.0037876730784773827,
      "learning_rate": 2.8891111111111114e-06,
      "loss": 0.004,
      "step": 77000
    },
    {
      "epoch": 3.4266666666666667,
      "grad_norm": 0.004320255480706692,
      "learning_rate": 2.866888888888889e-06,
      "loss": 0.0033,
      "step": 77100
    },
    {
      "epoch": 3.431111111111111,
      "grad_norm": 0.0044092414900660515,
      "learning_rate": 2.8446666666666666e-06,
      "loss": 0.0088,
      "step": 77200
    },
    {
      "epoch": 3.4355555555555557,
      "grad_norm": 0.005944984965026379,
      "learning_rate": 2.8224444444444447e-06,
      "loss": 0.0046,
      "step": 77300
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.007758188061416149,
      "learning_rate": 2.8002222222222223e-06,
      "loss": 0.0033,
      "step": 77400
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.0019370284862816334,
      "learning_rate": 2.7780000000000003e-06,
      "loss": 0.0104,
      "step": 77500
    },
    {
      "epoch": 3.448888888888889,
      "grad_norm": 0.0032478950452059507,
      "learning_rate": 2.755777777777778e-06,
      "loss": 0.0162,
      "step": 77600
    },
    {
      "epoch": 3.453333333333333,
      "grad_norm": 0.0054454561322927475,
      "learning_rate": 2.733555555555556e-06,
      "loss": 0.0045,
      "step": 77700
    },
    {
      "epoch": 3.457777777777778,
      "grad_norm": 0.004563977010548115,
      "learning_rate": 2.7113333333333336e-06,
      "loss": 0.0047,
      "step": 77800
    },
    {
      "epoch": 3.462222222222222,
      "grad_norm": 0.0038652715738862753,
      "learning_rate": 2.6891111111111112e-06,
      "loss": 0.0002,
      "step": 77900
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 0.06715042889118195,
      "learning_rate": 2.666888888888889e-06,
      "loss": 0.0128,
      "step": 78000
    },
    {
      "epoch": 3.471111111111111,
      "grad_norm": 0.004054081626236439,
      "learning_rate": 2.644666666666667e-06,
      "loss": 0.0002,
      "step": 78100
    },
    {
      "epoch": 3.4755555555555557,
      "grad_norm": 0.005805221851915121,
      "learning_rate": 2.6224444444444445e-06,
      "loss": 0.0033,
      "step": 78200
    },
    {
      "epoch": 3.48,
      "grad_norm": 0.008601679466664791,
      "learning_rate": 2.6002222222222225e-06,
      "loss": 0.0082,
      "step": 78300
    },
    {
      "epoch": 3.4844444444444447,
      "grad_norm": 0.010518945753574371,
      "learning_rate": 2.578e-06,
      "loss": 0.0165,
      "step": 78400
    },
    {
      "epoch": 3.488888888888889,
      "grad_norm": 0.002862706780433655,
      "learning_rate": 2.555777777777778e-06,
      "loss": 0.0003,
      "step": 78500
    },
    {
      "epoch": 3.493333333333333,
      "grad_norm": 0.004411696922034025,
      "learning_rate": 2.5335555555555558e-06,
      "loss": 0.0024,
      "step": 78600
    },
    {
      "epoch": 3.497777777777778,
      "grad_norm": 0.0028073859866708517,
      "learning_rate": 2.5113333333333334e-06,
      "loss": 0.0002,
      "step": 78700
    },
    {
      "epoch": 3.502222222222222,
      "grad_norm": 0.0030206318479031324,
      "learning_rate": 2.4891111111111114e-06,
      "loss": 0.0096,
      "step": 78800
    },
    {
      "epoch": 3.506666666666667,
      "grad_norm": 0.0299293901771307,
      "learning_rate": 2.466888888888889e-06,
      "loss": 0.0071,
      "step": 78900
    },
    {
      "epoch": 3.511111111111111,
      "grad_norm": 0.0028868026565760374,
      "learning_rate": 2.4446666666666667e-06,
      "loss": 0.0091,
      "step": 79000
    },
    {
      "epoch": 3.5155555555555553,
      "grad_norm": 0.004020636435598135,
      "learning_rate": 2.4224444444444447e-06,
      "loss": 0.0002,
      "step": 79100
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.005731787066906691,
      "learning_rate": 2.4002222222222223e-06,
      "loss": 0.0081,
      "step": 79200
    },
    {
      "epoch": 3.5244444444444447,
      "grad_norm": 0.006474491208791733,
      "learning_rate": 2.3780000000000004e-06,
      "loss": 0.0002,
      "step": 79300
    },
    {
      "epoch": 3.528888888888889,
      "grad_norm": 0.001950186095200479,
      "learning_rate": 2.355777777777778e-06,
      "loss": 0.0053,
      "step": 79400
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 0.0024381070397794247,
      "learning_rate": 2.333555555555556e-06,
      "loss": 0.0069,
      "step": 79500
    },
    {
      "epoch": 3.537777777777778,
      "grad_norm": 0.017298737540841103,
      "learning_rate": 2.3113333333333336e-06,
      "loss": 0.0047,
      "step": 79600
    },
    {
      "epoch": 3.542222222222222,
      "grad_norm": 0.007981271483004093,
      "learning_rate": 2.2891111111111112e-06,
      "loss": 0.0151,
      "step": 79700
    },
    {
      "epoch": 3.546666666666667,
      "grad_norm": 0.0062966481782495975,
      "learning_rate": 2.266888888888889e-06,
      "loss": 0.0132,
      "step": 79800
    },
    {
      "epoch": 3.551111111111111,
      "grad_norm": 0.0021830317564308643,
      "learning_rate": 2.244666666666667e-06,
      "loss": 0.0002,
      "step": 79900
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 0.0036603878252208233,
      "learning_rate": 2.2224444444444445e-06,
      "loss": 0.0139,
      "step": 80000
    },
    {
      "epoch": 3.56,
      "grad_norm": 3.7257351875305176,
      "learning_rate": 2.2002222222222225e-06,
      "loss": 0.0111,
      "step": 80100
    },
    {
      "epoch": 3.5644444444444443,
      "grad_norm": 0.020978650078177452,
      "learning_rate": 2.178e-06,
      "loss": 0.0087,
      "step": 80200
    },
    {
      "epoch": 3.568888888888889,
      "grad_norm": 0.013890812173485756,
      "learning_rate": 2.155777777777778e-06,
      "loss": 0.0084,
      "step": 80300
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 0.009368060156702995,
      "learning_rate": 2.133555555555556e-06,
      "loss": 0.0104,
      "step": 80400
    },
    {
      "epoch": 3.5777777777777775,
      "grad_norm": 0.006567428354173899,
      "learning_rate": 2.1113333333333334e-06,
      "loss": 0.0046,
      "step": 80500
    },
    {
      "epoch": 3.582222222222222,
      "grad_norm": 0.006861028261482716,
      "learning_rate": 2.089111111111111e-06,
      "loss": 0.0002,
      "step": 80600
    },
    {
      "epoch": 3.586666666666667,
      "grad_norm": 0.0045035467483103275,
      "learning_rate": 2.066888888888889e-06,
      "loss": 0.0038,
      "step": 80700
    },
    {
      "epoch": 3.591111111111111,
      "grad_norm": 0.00416712649166584,
      "learning_rate": 2.0446666666666667e-06,
      "loss": 0.0045,
      "step": 80800
    },
    {
      "epoch": 3.5955555555555554,
      "grad_norm": 0.0077224113047122955,
      "learning_rate": 2.0224444444444447e-06,
      "loss": 0.0023,
      "step": 80900
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.008923676796257496,
      "learning_rate": 2.0002222222222223e-06,
      "loss": 0.0087,
      "step": 81000
    },
    {
      "epoch": 3.6044444444444443,
      "grad_norm": 0.015225658193230629,
      "learning_rate": 1.9780000000000004e-06,
      "loss": 0.0044,
      "step": 81100
    },
    {
      "epoch": 3.608888888888889,
      "grad_norm": 0.004775518551468849,
      "learning_rate": 1.955777777777778e-06,
      "loss": 0.0036,
      "step": 81200
    },
    {
      "epoch": 3.6133333333333333,
      "grad_norm": 0.005469713360071182,
      "learning_rate": 1.9335555555555556e-06,
      "loss": 0.0021,
      "step": 81300
    },
    {
      "epoch": 3.6177777777777775,
      "grad_norm": 3.5262110233306885,
      "learning_rate": 1.9113333333333332e-06,
      "loss": 0.0116,
      "step": 81400
    },
    {
      "epoch": 3.6222222222222222,
      "grad_norm": 0.006442046724259853,
      "learning_rate": 1.8891111111111113e-06,
      "loss": 0.0031,
      "step": 81500
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 0.0019413300324231386,
      "learning_rate": 1.8668888888888889e-06,
      "loss": 0.0008,
      "step": 81600
    },
    {
      "epoch": 3.631111111111111,
      "grad_norm": 0.0030334205366671085,
      "learning_rate": 1.844666666666667e-06,
      "loss": 0.0021,
      "step": 81700
    },
    {
      "epoch": 3.6355555555555554,
      "grad_norm": 0.005153378006070852,
      "learning_rate": 1.8224444444444445e-06,
      "loss": 0.0001,
      "step": 81800
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.0019936254248023033,
      "learning_rate": 1.8002222222222223e-06,
      "loss": 0.0087,
      "step": 81900
    },
    {
      "epoch": 3.6444444444444444,
      "grad_norm": 0.07620509713888168,
      "learning_rate": 1.7780000000000004e-06,
      "loss": 0.0028,
      "step": 82000
    },
    {
      "epoch": 3.648888888888889,
      "grad_norm": 0.014321524649858475,
      "learning_rate": 1.755777777777778e-06,
      "loss": 0.0076,
      "step": 82100
    },
    {
      "epoch": 3.6533333333333333,
      "grad_norm": 0.01439262367784977,
      "learning_rate": 1.7335555555555558e-06,
      "loss": 0.0002,
      "step": 82200
    },
    {
      "epoch": 3.6577777777777776,
      "grad_norm": 0.003944149240851402,
      "learning_rate": 1.7113333333333334e-06,
      "loss": 0.0094,
      "step": 82300
    },
    {
      "epoch": 3.6622222222222223,
      "grad_norm": 0.013911103829741478,
      "learning_rate": 1.6891111111111113e-06,
      "loss": 0.0169,
      "step": 82400
    },
    {
      "epoch": 3.6666666666666665,
      "grad_norm": 0.001839274074882269,
      "learning_rate": 1.666888888888889e-06,
      "loss": 0.0034,
      "step": 82500
    },
    {
      "epoch": 3.671111111111111,
      "grad_norm": 0.00797822792083025,
      "learning_rate": 1.644666666666667e-06,
      "loss": 0.0054,
      "step": 82600
    },
    {
      "epoch": 3.6755555555555555,
      "grad_norm": 0.0022870732937008142,
      "learning_rate": 1.6224444444444445e-06,
      "loss": 0.0097,
      "step": 82700
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.0025820566806942225,
      "learning_rate": 1.6002222222222224e-06,
      "loss": 0.005,
      "step": 82800
    },
    {
      "epoch": 3.6844444444444444,
      "grad_norm": 0.0032854785677045584,
      "learning_rate": 1.5780000000000002e-06,
      "loss": 0.0098,
      "step": 82900
    },
    {
      "epoch": 3.688888888888889,
      "grad_norm": 0.001013695728033781,
      "learning_rate": 1.555777777777778e-06,
      "loss": 0.0002,
      "step": 83000
    },
    {
      "epoch": 3.6933333333333334,
      "grad_norm": 0.0014408240094780922,
      "learning_rate": 1.5335555555555556e-06,
      "loss": 0.0001,
      "step": 83100
    },
    {
      "epoch": 3.6977777777777776,
      "grad_norm": 0.006128871347755194,
      "learning_rate": 1.5113333333333334e-06,
      "loss": 0.0001,
      "step": 83200
    },
    {
      "epoch": 3.7022222222222223,
      "grad_norm": 0.002649811329320073,
      "learning_rate": 1.4891111111111113e-06,
      "loss": 0.0022,
      "step": 83300
    },
    {
      "epoch": 3.7066666666666666,
      "grad_norm": 0.002548396587371826,
      "learning_rate": 1.466888888888889e-06,
      "loss": 0.0054,
      "step": 83400
    },
    {
      "epoch": 3.7111111111111112,
      "grad_norm": 0.037085823714733124,
      "learning_rate": 1.4446666666666667e-06,
      "loss": 0.0075,
      "step": 83500
    },
    {
      "epoch": 3.7155555555555555,
      "grad_norm": 0.00389097654260695,
      "learning_rate": 1.4224444444444445e-06,
      "loss": 0.0117,
      "step": 83600
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.0038207906764000654,
      "learning_rate": 1.4002222222222224e-06,
      "loss": 0.0112,
      "step": 83700
    },
    {
      "epoch": 3.7244444444444444,
      "grad_norm": 0.0027917353436350822,
      "learning_rate": 1.3780000000000002e-06,
      "loss": 0.0001,
      "step": 83800
    },
    {
      "epoch": 3.728888888888889,
      "grad_norm": 0.0013170941965654492,
      "learning_rate": 1.3557777777777778e-06,
      "loss": 0.0154,
      "step": 83900
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 0.0014832073356956244,
      "learning_rate": 1.3335555555555556e-06,
      "loss": 0.0025,
      "step": 84000
    },
    {
      "epoch": 3.7377777777777776,
      "grad_norm": 0.0029141423292458057,
      "learning_rate": 1.3113333333333332e-06,
      "loss": 0.0001,
      "step": 84100
    },
    {
      "epoch": 3.7422222222222223,
      "grad_norm": 0.003375733969733119,
      "learning_rate": 1.2891111111111113e-06,
      "loss": 0.0081,
      "step": 84200
    },
    {
      "epoch": 3.7466666666666666,
      "grad_norm": 0.0017863557441160083,
      "learning_rate": 1.266888888888889e-06,
      "loss": 0.0039,
      "step": 84300
    },
    {
      "epoch": 3.7511111111111113,
      "grad_norm": 0.001656136941164732,
      "learning_rate": 1.2446666666666667e-06,
      "loss": 0.0056,
      "step": 84400
    },
    {
      "epoch": 3.7555555555555555,
      "grad_norm": 0.0022063583601266146,
      "learning_rate": 1.2224444444444445e-06,
      "loss": 0.0035,
      "step": 84500
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.0030235713347792625,
      "learning_rate": 1.2002222222222224e-06,
      "loss": 0.0044,
      "step": 84600
    },
    {
      "epoch": 3.7644444444444445,
      "grad_norm": 0.002549548167735338,
      "learning_rate": 1.1780000000000002e-06,
      "loss": 0.0088,
      "step": 84700
    },
    {
      "epoch": 3.7688888888888887,
      "grad_norm": 0.00315722543746233,
      "learning_rate": 1.1557777777777778e-06,
      "loss": 0.005,
      "step": 84800
    },
    {
      "epoch": 3.7733333333333334,
      "grad_norm": 0.0014120959676802158,
      "learning_rate": 1.1335555555555556e-06,
      "loss": 0.0001,
      "step": 84900
    },
    {
      "epoch": 3.7777777777777777,
      "grad_norm": 0.006699143908917904,
      "learning_rate": 1.1113333333333335e-06,
      "loss": 0.0015,
      "step": 85000
    },
    {
      "epoch": 3.7822222222222224,
      "grad_norm": 0.0030829403549432755,
      "learning_rate": 1.0891111111111113e-06,
      "loss": 0.0051,
      "step": 85100
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 0.004310928750783205,
      "learning_rate": 1.066888888888889e-06,
      "loss": 0.0023,
      "step": 85200
    },
    {
      "epoch": 3.7911111111111113,
      "grad_norm": 0.003898079041391611,
      "learning_rate": 1.0446666666666667e-06,
      "loss": 0.0053,
      "step": 85300
    },
    {
      "epoch": 3.7955555555555556,
      "grad_norm": 0.002017777180299163,
      "learning_rate": 1.0224444444444446e-06,
      "loss": 0.0035,
      "step": 85400
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.002772677456960082,
      "learning_rate": 1.0002222222222224e-06,
      "loss": 0.0068,
      "step": 85500
    },
    {
      "epoch": 3.8044444444444445,
      "grad_norm": 0.004184903111308813,
      "learning_rate": 9.78e-07,
      "loss": 0.0091,
      "step": 85600
    },
    {
      "epoch": 3.8088888888888888,
      "grad_norm": 0.0012995015131309628,
      "learning_rate": 9.557777777777778e-07,
      "loss": 0.0003,
      "step": 85700
    },
    {
      "epoch": 3.8133333333333335,
      "grad_norm": 0.002359784673899412,
      "learning_rate": 9.335555555555556e-07,
      "loss": 0.0133,
      "step": 85800
    },
    {
      "epoch": 3.8177777777777777,
      "grad_norm": 0.008966268040239811,
      "learning_rate": 9.113333333333334e-07,
      "loss": 0.0055,
      "step": 85900
    },
    {
      "epoch": 3.822222222222222,
      "grad_norm": 0.009130075573921204,
      "learning_rate": 8.891111111111112e-07,
      "loss": 0.0143,
      "step": 86000
    },
    {
      "epoch": 3.8266666666666667,
      "grad_norm": 0.015638191252946854,
      "learning_rate": 8.668888888888889e-07,
      "loss": 0.0096,
      "step": 86100
    },
    {
      "epoch": 3.8311111111111114,
      "grad_norm": 0.002275311155244708,
      "learning_rate": 8.446666666666667e-07,
      "loss": 0.0053,
      "step": 86200
    },
    {
      "epoch": 3.8355555555555556,
      "grad_norm": 0.013830170966684818,
      "learning_rate": 8.224444444444445e-07,
      "loss": 0.0055,
      "step": 86300
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.023637766018509865,
      "learning_rate": 8.002222222222223e-07,
      "loss": 0.0002,
      "step": 86400
    },
    {
      "epoch": 3.8444444444444446,
      "grad_norm": 0.0031364448368549347,
      "learning_rate": 7.78e-07,
      "loss": 0.0033,
      "step": 86500
    },
    {
      "epoch": 3.848888888888889,
      "grad_norm": 0.0025877144653350115,
      "learning_rate": 7.557777777777777e-07,
      "loss": 0.0117,
      "step": 86600
    },
    {
      "epoch": 3.8533333333333335,
      "grad_norm": 0.008177968673408031,
      "learning_rate": 7.335555555555556e-07,
      "loss": 0.0047,
      "step": 86700
    },
    {
      "epoch": 3.8577777777777778,
      "grad_norm": 0.003661408321931958,
      "learning_rate": 7.113333333333335e-07,
      "loss": 0.0044,
      "step": 86800
    },
    {
      "epoch": 3.862222222222222,
      "grad_norm": 0.002518664114177227,
      "learning_rate": 6.891111111111112e-07,
      "loss": 0.0001,
      "step": 86900
    },
    {
      "epoch": 3.8666666666666667,
      "grad_norm": 0.002802590373903513,
      "learning_rate": 6.66888888888889e-07,
      "loss": 0.0048,
      "step": 87000
    },
    {
      "epoch": 3.871111111111111,
      "grad_norm": 0.0021987250074744225,
      "learning_rate": 6.446666666666667e-07,
      "loss": 0.0061,
      "step": 87100
    },
    {
      "epoch": 3.8755555555555556,
      "grad_norm": 0.0057746367529034615,
      "learning_rate": 6.224444444444445e-07,
      "loss": 0.0076,
      "step": 87200
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.0036315000616014004,
      "learning_rate": 6.002222222222223e-07,
      "loss": 0.0094,
      "step": 87300
    },
    {
      "epoch": 3.8844444444444446,
      "grad_norm": 0.01026283111423254,
      "learning_rate": 5.78e-07,
      "loss": 0.007,
      "step": 87400
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 0.002740349853411317,
      "learning_rate": 5.557777777777778e-07,
      "loss": 0.0002,
      "step": 87500
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 0.005671665072441101,
      "learning_rate": 5.335555555555557e-07,
      "loss": 0.0002,
      "step": 87600
    },
    {
      "epoch": 3.897777777777778,
      "grad_norm": 2.900221347808838,
      "learning_rate": 5.113333333333334e-07,
      "loss": 0.0083,
      "step": 87700
    },
    {
      "epoch": 3.902222222222222,
      "grad_norm": 0.007297990377992392,
      "learning_rate": 4.891111111111112e-07,
      "loss": 0.0038,
      "step": 87800
    },
    {
      "epoch": 3.9066666666666667,
      "grad_norm": 0.00486758491024375,
      "learning_rate": 4.6688888888888893e-07,
      "loss": 0.0109,
      "step": 87900
    },
    {
      "epoch": 3.911111111111111,
      "grad_norm": 0.0033603012561798096,
      "learning_rate": 4.446666666666667e-07,
      "loss": 0.02,
      "step": 88000
    },
    {
      "epoch": 3.9155555555555557,
      "grad_norm": 0.006873942445963621,
      "learning_rate": 4.224444444444445e-07,
      "loss": 0.004,
      "step": 88100
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.005136738996952772,
      "learning_rate": 4.0022222222222225e-07,
      "loss": 0.0002,
      "step": 88200
    },
    {
      "epoch": 3.924444444444444,
      "grad_norm": 0.0028789262287318707,
      "learning_rate": 3.78e-07,
      "loss": 0.0188,
      "step": 88300
    },
    {
      "epoch": 3.928888888888889,
      "grad_norm": 0.024242661893367767,
      "learning_rate": 3.557777777777778e-07,
      "loss": 0.0069,
      "step": 88400
    },
    {
      "epoch": 3.9333333333333336,
      "grad_norm": 0.0017009606817737222,
      "learning_rate": 3.3355555555555557e-07,
      "loss": 0.0002,
      "step": 88500
    },
    {
      "epoch": 3.937777777777778,
      "grad_norm": 0.0024152626283466816,
      "learning_rate": 3.1133333333333334e-07,
      "loss": 0.0002,
      "step": 88600
    },
    {
      "epoch": 3.942222222222222,
      "grad_norm": 0.005731679033488035,
      "learning_rate": 2.891111111111111e-07,
      "loss": 0.0003,
      "step": 88700
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 0.0069435532204806805,
      "learning_rate": 2.6688888888888894e-07,
      "loss": 0.0111,
      "step": 88800
    },
    {
      "epoch": 3.951111111111111,
      "grad_norm": 0.008074200712144375,
      "learning_rate": 2.446666666666667e-07,
      "loss": 0.0108,
      "step": 88900
    },
    {
      "epoch": 3.9555555555555557,
      "grad_norm": 0.004529798869043589,
      "learning_rate": 2.2244444444444446e-07,
      "loss": 0.0003,
      "step": 89000
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.005135267041623592,
      "learning_rate": 2.0022222222222223e-07,
      "loss": 0.0043,
      "step": 89100
    },
    {
      "epoch": 3.964444444444444,
      "grad_norm": 0.014340662397444248,
      "learning_rate": 1.78e-07,
      "loss": 0.0002,
      "step": 89200
    },
    {
      "epoch": 3.968888888888889,
      "grad_norm": 0.003623301861807704,
      "learning_rate": 1.5577777777777778e-07,
      "loss": 0.0087,
      "step": 89300
    },
    {
      "epoch": 3.9733333333333336,
      "grad_norm": 0.00921696051955223,
      "learning_rate": 1.3355555555555558e-07,
      "loss": 0.0048,
      "step": 89400
    },
    {
      "epoch": 3.977777777777778,
      "grad_norm": 0.002027261769399047,
      "learning_rate": 1.1133333333333335e-07,
      "loss": 0.0083,
      "step": 89500
    },
    {
      "epoch": 3.982222222222222,
      "grad_norm": 0.013382475823163986,
      "learning_rate": 8.911111111111111e-08,
      "loss": 0.0029,
      "step": 89600
    },
    {
      "epoch": 3.986666666666667,
      "grad_norm": 0.0019243943970650434,
      "learning_rate": 6.68888888888889e-08,
      "loss": 0.0048,
      "step": 89700
    },
    {
      "epoch": 3.991111111111111,
      "grad_norm": 0.006033934652805328,
      "learning_rate": 4.466666666666667e-08,
      "loss": 0.0001,
      "step": 89800
    },
    {
      "epoch": 3.9955555555555557,
      "grad_norm": 0.008204835467040539,
      "learning_rate": 2.2444444444444446e-08,
      "loss": 0.01,
      "step": 89900
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.0033092000521719456,
      "learning_rate": 2.2222222222222224e-10,
      "loss": 0.0002,
      "step": 90000
    }
  ],
  "logging_steps": 100,
  "max_steps": 90000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.736168976384e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
